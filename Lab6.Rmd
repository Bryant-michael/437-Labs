---
title: 'Lab Assignment #6'
author: "Vanessa Avilez, Michael Bryant, Phuong Traceyle"
date: "Due March 15, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce a few different classification strategies. In this lab we will work with k-nearest neighbors, logistic regression, and their generalizations to more than 2 response categories.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(class) # knn
library(nycflights13) # Problem 3
library(nnet) # multinomial logistic regression
library(workflows)
library(parsnip)
library(recipes)
library(rsample)
library(broom)
```

This lab assignment is worth a total of **20 points**.

# Problem 1: Example Code 

## Part a (Code: 1.5 pts)

Run the code in ISLR Labs 4.7.1, 4.7.2, and 4.7.6. Put each chunk from the textbook in its own chunk.

```{r 4.7.1 p1}
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
```

```{r 4.7.1 p2}
#cor(Smarket)
cor(Smarket[, -9])
```

```{r 4.7.1 p3}
attach(Smarket)
plot(Volume)
```

```{r 4.7.2 p1}
glm.fits <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
  data = Smarket, family = binomial
)

summary(glm.fits)
```

```{r 4.7.2 p2}
coef(glm.fits)
summary(glm.fits)$coef
summary(glm.fits)$coef[,4]
```

```{r 4.7.2 p3}
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
contrasts(Direction)
```

```{r 4.7.2 p4}
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
```

```{r 4.7.2 p5}
table(glm.pred, Direction)
(507 + 145) / 1250
mean(glm.pred == Direction)
```

```{r 4.7.2 p6}
train <- (Year < 2005)
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
```

```{r 4.7.2 p7}
glm.fits <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume ,
  data = Smarket , family = binomial , subset = train
)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
```

```{r 4.7.2 p8}
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
```

```{r 4.7.2 p9}
glm.fits <- glm(Direction ~ Lag1 + Lag2 , data = Smarket ,
  family = binomial , subset = train)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
106/ (106 +76)
```

```{r 4.7.2 p10}
predict(glm.fits, newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), 
                                       type = "response")
```

```{r 4.7.6 p1}
train.X <- cbind(Lag1, Lag2)[train, ]
test.X <- cbind(Lag1, Lag2)[!train, ]
train.Direction <- Direction[train]
```

```{r 4.7.6 p2}
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, Direction.2005)
(83 + 43) /252
```

```{r 4.7.6 p3}
knn.pred <- knn(train.X, test.X, train.Direction, k=3)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```

```{r 4.7.6 p4}
dim(Caravan)
attach(Caravan)
summary(Purchase)
348 / 5822
```

```{r 4.7.6 p5}
standardized.X <- scale (Caravan[, -86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
```

```{r 4.7.6 p6}
test <- 1:1000
train.X <- standardized.X[-test,]
test.X <- standardized.X[test, ]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Y, k =1)
mean(test.Y != knn.pred)
mean(test.Y != "No")
```

```{r 4.7.6 p7}
table(knn.pred, test.Y)
9 / (68 + 9)
```

```{r 4.7.6 p8}
knn.pred <- knn (train.X, test.X, train.Y, k = 3)
table (knn.pred , test.Y)
5/26
knn.pred <- knn (train.X, test.X, train.Y, k = 5)
table (knn.pred , test.Y)
4/15
```

```{r 4.7.6 p9}
glm.fits <- glm(Purchase ~ ., data = Caravan ,
  family = binomial , subset = -test)
glm.probs <- predict (glm.fits , Caravan[test , ],
  type = "response")
glm.pred <- rep ("No", 1000)
glm.pred[glm.probs > .5] <- " Yes "
table (glm.pred , test.Y)
glm.pred <- rep ("No", 1000)
glm.pred[glm.probs > .25] <- " Yes "
table (glm.pred , test.Y)
11/ (22+11)
```

## Part b (Explanation: 1 pt)

In Lab 4.7.6, both the k-nn model with `k=5` and the logistic regression model using `probs > 0.5` as the classification threshold produced a test error rate (misclassification rate) of 6.6% using the `Caravan` dataset. Explain why we would prefer to use the k-nn model.

We would prefer to use the knn model because it is the easier model to use, and we know the two models will act the same. 

## Part c (Explanation: 1 pt)

If you forget to include the `family = binomial` argument when calling `glm`, what does R try to do instead of running a logistic regression? It may be useful to look up the documentation for the `glm` function, look up the part of the Logistic Regression Class Activity where we actually did this, and/or read Section 4.6.3 of the textbook.

If we do not specify a `family` then the default is Gaussian so  will try to use this instead of a logisitic regression.

# Problem 2: Analysis of iris Dataset: virginica vs versicolor

The `iris` dataset is a well-known benchmark dataset for evaluating the performance of new classification algorithms on small sets of data. It is found in the `datasets` package (which should automatically be loaded when you start R).

The dataset consists of five variables: the response variable (`Species`) and four explanatory variables (`Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`, each measured in cm). Because the species are fairly distinct in terms of the four explanatory variables, we should expect near-100% accuracy with any decent classification model.

In this problem, we will do all of our modeling without using the `tidymodels` packages.
```{r 2a}
iris_vv <- iris %>% filter(Species == c("virginica","versicolor"))
ggplot(iris_vv,aes(x=Petal.Length, y=Sepal.Length, col=Species)) + geom_point()

```


## Part b (Code: 2 pts)

Prepare your dataset for the k-nearest neighbors algorithm by doing the following:


### Step 1: Create the training and holdout sets

Randomly select 20 observations to be in the holdout set. The remaining observations are in the training set. Call these new datasets `iris.train` and `iris.test`.

HINT: The easiest way to do this is to `sample` from the indices and subset accordingly.

```{r 2b step 1}
set.seed(437)
test.rows <- sample(nrow(iris_vv), 20, replace = FALSE)
iris.train <- iris_vv[-test.rows,]
iris.test <- iris_vv[test.rows,]
```

### Step 2: Scale the predictor variables

Create two new datasets, `iris.train.X` and `iris.test.X`, containing the explanatory variables in each set. Remember that you need to scale the variables in both the training and holdout sets based on the training set!

```{r 2b step 1}
iris.train.x <- iris.train %>% 
  dplyr::select(Petal.Length, Petal.Width, Sepal.Length, Sepal.Width) %>% 
  scale()
iris.test.x <- iris.test %>% 
  dplyr::select(Petal.Length, Petal.Width, Sepal.Length, Sepal.Width) %>%
  mutate(
    Petal.Length = (Petal.Length - mean(iris.train$Petal.Length))/sd(iris.train$Petal.Length),
    Petal.Width = (Petal.Width - mean(iris.train$Petal.Width))/sd(iris.train$Petal.Width),
    Sepal.Length = (Sepal.Length - mean(iris.train$Sepal.Length))/sd(iris.train$Sepal.Length),
    Sepal.Width = (Sepal.Width - mean(iris.train$Sepal.Width))/sd(iris.train$Sepal.Width),
  )
```

## Part c (Code: 2 pts; Explanation: 0.5 pts)

With the `knn` function, use 5-nearest neighbors to predict the species of each observation in the `iris.test` holdout set. Create a `table` showing the predicted and actual species in the holdout set. What is the prediction accuracy of this model?
iris.train$Species
```{r 2c}
set.seed(437)
k1 <- knn(train = iris.train.x, test = iris.test.x, 
          cl = iris.train$Species, k = 5)
table(k1, iris.test$Species)
(9 + 8)/20
```

-> We have a prediction accuracy of 85%.

## Part d (Code: 1 pt; Explanation: 1 pt)

Fit a logistic regression model on the training set predicting `Species` from the four explanatory variables. Unlike with k-nearest neighbors, you do not need to scale the variables. Note that there may be some weirdness with figuring out the reference level for `Species`, so you may want to use the `relevel` function to set that before fitting the model.

Use the `summary` or `coef` function to obtain the coefficient estimates, and write out the equation of the fitted logistic regression model.

```{r 2d}
iris.train$Species <- relevel(iris.train$Species, ref = "versicolor")
logr_iris <- glm(Species ~ Sepal.Width + Sepal.Length + Petal.Width + Petal.Length, data = iris.train,
                     family = "binomial")
summary(logr_iris)
```

logit(p) = -51.731 - 8.959(Sepal.Width) - 60.689(Sepal.Length) + 96.297(Petal.Width) + 59.73(Petal.Length)

## Part e (Code: 2 pts; Explanation: 0.5 pts)

Use the `predict` function to obtain predictions for the `iris.test` holdout set. Remember to include the argument `type = "response"` to output predictions as probabilities!

Predict the class of each flower in the holdout set using a decision boundary of 0.5 (i.e., if the predicted probability of being in *versicolor* vs. *virginica* is above 0.5, predict the flower to be in *versicolor*). Create a `table` showing the predicted and actual species in the holdout set. What is the prediction accuracy of this model on the holdout set?

```{r 2e}
predicted_probs_m <- predict(logr_iris, newdata = iris.test, type = "response")

p.threshold <- 0.5

#Use if_else from dplyr to do the prediction
predicted_category_m <- if_else(predicted_probs_m < p.threshold, "versicolor", "virginica")

logr_prediction_m <- tibble(Sepal.Length = iris.test$Sepal.Length,
                           Sepal.Width = iris.test$Sepal.Width,
                           Petal.Length = iris.test$Petal.Length,
                           Petal.Width = iris.test$Petal.Width,
                           Species = iris.test$Species,
                          pred_prob = predicted_probs_m, pred_class = predicted_category_m)

logr_prediction_m

table(logr_prediction_m$pred_class, logr_prediction_m$Species, 
      dnn = c("Predicted","Actual"))
19/20
```

-> 95% accurate
# Problem 3: Analysis of flights Dataset

There are three major airports that serve the New York City metro area: John F. Kennedy International Airport (JFK), Newark Liberty International Airport (EWR), and LaGuardia Airport (LGA). Newark is a United Airlines hub, JFK is an American Airlines hub, while Delta has hubs at both JFK and LGA.

The `flights` dataset contains flights from 2013 that departed one of those three airports. Here we filter the dataset to include only flights that departed on one of those three carriers. 

```{r filter flights}
flights2 <- flights %>% filter(
  carrier %in% c("UA", "AA", "DL"),
  !is.na(dep_delay)
) %>%
  mutate(origin = as.factor(origin))
```

Let's try to predict which airport a flight departed from into based on the carrier as well as the distance the flight has covered and the departure delay. In this problem we'll do everything the `tidymodels` way.

## Part a (Code: 1 pt)

First, we'll use 3-nearest neighbors to predict the departure airport of each observation in the `flights_test` holdout set.

```{r pre-processing and model spec}
knn_model <- nearest_neighbor(mode = "classification", neighbors = 3, dist_power = 2)

set.seed(2222)
flights_split <- initial_split(flights2, prop = 0.8)
flights_train <- training(flights_split)
flights_test <- testing(flights_split)

knn_prep <- recipe(
  origin ~ carrier + distance + dep_delay, # response ~ predictors
  data = flights_train
) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) # center and scale numeric predictors

flights_knn <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(knn_prep)
```

In the chunk below, write code that will `fit` the model on the training set. (If your model runs for ~3 seconds without an error message, start on part (b) while you wait for the chunk to churn.)

```{r fitting k-nn}
flights_knn_fit <- fit(flights_knn, data = flights_train)
flights_knn_fit
```

Next, use the `augment` function to create the tibble `flights_knn_predictions` containing the predictions for each flight in the `flights_test` dataset. 

```{r predicting with broom::augment}
flights_knn_predictions <- broom::augment(flights_knn_fit, new_data = flights_test, type = "prob")

flights_knn_predictions %>% dplyr::select(
  origin,
  .pred_class,
  .pred_EWR,
  .pred_JFK,
  .pred_LGA
) %>% head(10)
```

## Part b (Explanation: 1 pt)

While you wait for your code to run, as best you can, answer the following questions:

* How did we ensure that roughly 20% of the data was in the holdout set, even though we didn't know the exact size of my dataset?
When we initially split the dataset `flight` we set the prob = 0.8, which is the proportion of data to be retained for analysis. So 80% was used for analysis and 20% for testing.  

* What does the `step_dummy` function do, and why did we need to do it?
The `step_dummy` function converts nominal data (ie character factors) into a numerical binary model terms for the levels of the original data. it need to be used in the recipe because tidymodel will not run with character/factor variables. 

## Part c (Code: 0.5 pts; Explanation: 0.5 pts)

Now let's use multinomial logistic regression to do the same predictions. Multinomial logistic regression requires much less fuss than k-nearest neighbors when it comes to data prep. Remember, since this uses *regression* frameworks, rescaling the variables just messes with intercept and slopes! 

The two main functions I use to do this are `VGAM::vglm` and `nnet::multinom`. I find that `vglm` is more generalizable, but it doesn't play nice with `tidymodels`, so we'll use `multinom` instead.

```{r fit multinomial}
# just run this - it all works
multinom_model <- multinom_reg(mode = "classification") # everything else is default

multinom_prep <- recipe(
  origin ~ carrier + distance + dep_delay, # response ~ predictors
  data = flights_train
) %>%
  step_dummy(all_nominal_predictors())

flights_multinom <- workflow() %>%
  add_model(multinom_model) %>%
  add_recipe(multinom_prep)
```

In the chunk below, write code that will `fit` the model on the training set and store the result in the object `flights_multinom_fit`.

```{r fit the model}
flights_multinom_fit <- fit(flights_multinom, data = flights_train)
flights_multinom_fit
```

What reference (baseline) level was chosen?
The reference level is EWR, because it was first alphabetically. 

## Part d (Code: 0.5 pts)

Getting the coefficients out as a separate R object is a bit of a pain, but we can do it:

```{r multinomial coefficients}
multinom_coef <- extract_fit_engine(flights_multinom_fit) %>% coef()
print(multinom_coef)
```

To interpret our coefficients, we can usually hack our way to *softmax* coding by adding a zero row (corresponding to the reference/baseline level) at the very top of the coefficient matrix. Then we can look at the effect of a change in the predictor variable on the log-odds of being in any response group relative to any other response group.

```{r coef matrix}
coef_softmax <- rbind(EWR = numeric(5),
                      multinom_coef)
print(coef_softmax)
```

Now, for two flights with the same carrier and distance, a one-mile increase in distance is associated with a 0.0017 increase in the log-odds of being from JFK as compared to EWR, and a 0.0006 decrease in the log-odds of being from LGA as compared to EWR. By subtracting the two nonzero slopes, we find that a one-mile increase in distance is associated with a $0.0017 - (-0.0006)$ = 0.0023 increase in the log-odds of being from JFK as compared to LGA.

Inference with a multinomial logistic regression model is possible by calling the `multinom` function directly (tidymodels appears to drop the standard errors from the output), but instead we're going to instead focus on prediction.

If we use the `multinom` function directly, then we can use the `predict` function just like in linear and logistic regression, but we have to use either `class` or `probs` as our `type` argument - we can't use `response` and then pick an arbitrary boundary threshold, since now we have multiple boundaries between different groups.

The `augment` function does away with this ridiculousness and gives us both class predictions and the predicted probability of being in each class without having the remember what argument to use for `type`. Using the `augment` function, create the tibble `flights_multinom_predictions` containing the predictions for each flight in the `flights_test` dataset. 

```{r multinom prediction}
flights_multinom_predictions <- broom::augment(flights_multinom_fit, new_data = flights_test, type = "prob")
```


```{r multinom predictions 43-45}
flights_multinom_predictions %>% dplyr::select(
  origin,
  .pred_class,
  .pred_EWR,
  .pred_JFK,
  .pred_LGA
) %>%
  slice(43:45) # equivalent to data[43:45,] but works with a pipe
```

When we have multi-class classification problems, it is possible that no class reaches a 50% probability (e.g., observation 44). In these situations, observations are still classified to the *most likely* airport, we just note that there is a lot more uncertainty about the prediction. 

## Part e (Code: 1 pt)

Using the `accuracy` function in the `yardstick` package, find the prediction accuracy of each model.

```{r accuracy}
accuracy(flights_knn_predictions, truth = origin, estimate = .pred_class)
accuracy(flights_multinom_predictions, truth = origin, estimate = .pred_class)
```


## Part f (Explanation: 1 pt)

Briefly discuss the advantages of using the 3-nearest neighbors algorithm over the multinomial logistic regression model on the `flights` data, and the advantages of using the multinomial logistic regression model over the 3-nearest neighbors algorithm. Prediction accuracy is important, but not the only thing you should compare.

KNN was an easy model to use however because it is using the 3 nearest, most of the predictions are 0 or 1. So the accuracy of it is 98%. With multinomial, we can use the coefficient matrix to understand the slopes, but the predictions are more varied which makes the accuracy 72%. SO knn makes better predictions than multinomial. 

