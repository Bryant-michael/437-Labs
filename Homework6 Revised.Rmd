---
title: 'Homework Assignment #6'
author: "Math 437 - Modern Data Analysis"
date: "Due May 12, 2023"
output: pdf_document
---

# Instructions

You should submit either two or three files:

1. You should write your solutions to the Applied Problem in this R Markdown file and submit the (.Rmd) file.
2. You should knit the final solution file to pdf and submit the pdf. If you are having trouble getting code chunks to run, add `eval = FALSE` to the chunks that do not run. If you are having trouble getting R Studio to play nice with your LaTeX distribution, I will begrudgingly accept an HTML file instead. 
3. Solutions to the Key Terms and Conceptual Problem can be submitted in a separate Word or pdf file or included in the same files as your solutions to the Applied Problem.

This homework assignment is worth a total of **35 points**.

# Key Terms (13 pts)

Read Chapters 8 (Sections 8.1-8.2.3), 10 (Sections 10.1-10.4, 10.6-10.7), and 12 (Sections 12.1-12.4) of Introduction to Statistical Learning, Second Edition. Based on your reading, answer the following questions.

1. How do you determine whether a node in a tree is a *terminal node* or an *internal node*?
-> Terminal nodes are leaves that are at the bottom of the tree. The points along the tree where the predictor space is split are internal nodes.
2. Briefly explain what is meant by the term *recursive binary splitting*.
-> Recursive binary splitting is a top-down, greedy approach. The approach is top-down because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.
3. Explain the similarity between *cost-complexity pruning* and the *lasso* model.
-> Equation 8.4 of the cost-complexity pruning is similar to the lasso model, in which a similar formulation that was used in order to control the complexity of a linear model.
4. Write out the equations for two different measures of *node purity* (or node impurity). Why are these measures preferred over simpler accuracy/inaccuracy measures?
-> $G = \sum_{k=1}^{K} \hat{p}_{mk} (1-\hat{p}_{mk})$ and $D = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$
These measures are preferred over simpler accuracy/inaccuracy measures because they are more sensitive to node purity.
5. Do decision trees perform better when there is a linear boundary or a highly nonlinear boundary? Why?
-> It depends on the problem at hand. If the relationship between the features and the response is well approximated by a linear model, then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure. If instead there is a highly nonlinear and complex relationship between the features and the response as indicated by model.
6. Describe the basic procedure in *bagging*. Could we use *bagging* with models that aren't trees (linear models, generative models, etc.)? Why or why not?
-> Bagging is a general-purpose procedure for reducing the variance of a statistical learning method. A natural way to reduce the variance and increase the test set accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. We could not use bagging with models that aren't trees because a procedure with low variance will yield similar results if applied repeatedly to distinct data sets.
7. Why might we use the *out-of-bag* prediction error to estimate the test error rate instead of using cross-validation? Describe how to compute it.
-> The out-of-bag approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous. We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we can average the predicted responses or can take a majority vote. This leads to a single OOB prediction for the ith observation.
8. Explain how to read the bar graph in Figure 8.9.
-> The bar graph in Figure 8.9 is a graphical representation of the variable importances in the Heart data. We see the mean decrease in Gini index for each variable, relative to the largest. The variables with the largest mean decrease in Gini index are Thal, Ca, and ChestPain.
9. When would we expect *random forests* to improve over a "regular" bagged trees model and why?
-> Random forests using $m = \sqrt{p}$ leads to a reduction in both test error and OOB error over bagging.
10. Briefly explain how *boosted trees* are grown.
-> Each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set. Boosting involves combining a large number of decision trees. Since the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient.
11. Briefly explain what is meant by the terms *hidden unit* and *hidden layer*.
-> A hidden unit is represnts the information that is not observed in the data. A hidden layer is where all of the activations of the hidden units are computed so they can be treated as inputs to determine the output.
12. What is an *activation*? Give two examples of activation functions.
-> An activation is a function of the inputs but it is more like a transformation of the original features in the hidden layer. Two activation functions are the sigmoid function and the RELU function. 
13. The machine learning community refers to *weights* and *bias* terms in neural networks. In statistical language, what do these terms correspond to?
-> Weights corresponds to the coefficients of the predictors and the bias refers to the intercept. 
14. In convolutional neural networks, what is the purpose of a *convolution layer*? What is the purpose of a *pooling layer*?
-> In convolutional neural networks, the convolution layer searches for patterns in the images. In convolutional neural networks, the pooling layer downsamples the patterns to select a prominent subset. 
15. Briefly explain how and why *data augmentation* is performed in image classification.
-> Data augmentation takes each image in the training set and replicates it many time while distoring the image in some way. This is to increase the training set with different example of the samr picture to help avoid the model from overfitting. 
16. Section 10.6 poses the question: "Should we discard all our older tools, and use deep learning on every problem with data?" Answer this question and explain your reasoning.
-> We should nto discard older tools and just use deep learning. In cases with a small data set, it will be easier to go with a simpler model that we can interpret well. There is no need to reinvent the wheel as they say, as the simpler model may be the best to go with if the deep learning model will end up peforming about the same. 
17. In the example in Section 10.6, the authors first used a lasso model to perform variable selection, then fit a linear model with the predictors selected. How did they use their final model to answer *inference* questions? Why did they do it that way?
-> They used their final model the see what the least squares coefficient estimates and the mean absolute error to see how well the model perform. They used the test set to make sure there is no selection bias affecting the model. 
18. Briefly explain how *gradient descent* methods work. Why is estimating the gradient relatively straightforward when (scaled) MSE is used as the objective function $R(\theta)$?
-> Gradient descent methods starts with an initial guess and starts going in one direction, against the gradient to get to the lowest point that it can. Estimating the gradient is straightforward when using a scale MSE because it is a closed form solution so computations are easier. 


19. What is a principal component *score* vector? How are the associated *loadings* computed?
-> A principal component score vector is a vector of the linear combinations of the sample feature values for their respective principal component. The associated loadings are computed using eigen decomposition.

20. The book mentions that eigen decomposition can be used to obtain the loadings (in practice most algorithms use a related technique, singular value decomposition, which is faster). What do the eigenvalues of the covariance matrix correspond to? What do the eigenvectors correspond to?
-> The eigenvalues correspond to the scale of the matrix while the eigenvectors correspond to the direction. 

21. What is a scree plot? Why is it useful?
-> A scree plot depicts the proportion of variance explained by each principal component. We use a scree plot to gauge how many principal components are required to visualize data.

22. What is the difference between data *missing at random* and *missing not at random* (or "not missing at random")? Is it appropriate to impute data that is missing at random? What about data that is not missing at random?
-> Missing at random involves missing data due to chance events rather than missing data due to inconvenience or informative reasons. It is appropriate to impute data missing at random to fill in gaps; however, imputing on not missing at random data is not suitable due to an informative reason behind the loss of data.

23. Give two application areas in which clustering techniques may be useful. You may use the ones identified in the book or describe your own.
-> One from the book is trying to cluster tissue samples from patients with breast cancer to discover subgroups of breast cancer. Another one could include market segmentation to identify subgroups of people who could be receptive to various types of advertising.

24. Why is it important to run k-means clustering from many different random initializations?
-> Since k-means chooses values randomly, successive runs can lead to varying results to potentially get better findings.

25. Briefly explain how to read a *dendrogram*.
-> The top of a dendrogram includes all of the observations and considering them similar at that point. With each branch, the observations' similarity gets finer and finer until they have as few similar observations as possible. Observations within one major branch are more similar to each other than observations between differing branches.

26. Many of the techniques we learned in this class have been rather "automatic" in the sense that there are fairly well-defined rules for selecting the "best" model and all we need "human judgment" for is determining which of several models "close enough to the best" we should actually use. This does not work for cluster analysis. Give at least two reasons why cluster analysis requires us to think a lot harder before settling on a "best" model.
-> Cluster analysis has many considerations that greatly impact the final results, such as type of linkage used or how many clusters to consider. This leads us to put an emphasis on finding the most useful and interpretable solution. Another consideration is how validated the final clusters are, and if they represent more noise than necessary.


# Conceptual Problems

## Conceptual Problem 1 (3 pts)

Consider the random variables $Y_1, Y_2, \ldots, Y_n$ to be independent (but not identically distributed) random variables with pmf given by

$$
P(Y_i = m|X = x) =
\begin{cases}
f_m(x_i), m = 0, 1, \ldots, 9\\
0, \text{otherwise}
\end{cases}
$$

where $f_m(x_i)$ are a collection of $m+1$ functions of predictor variables $X$ evaluated for observation $i$. The joint pmf of the $Y_i$'s is then:

$$
P(Y_1 = m_1, Y_2 = m_2, \ldots, Y_n = m_n) = \prod_{i=1}^n \prod_{m=0}^9 \left(f_m(x_i)\right)^{y_{im}}
$$

where $y_im$ is an indicator variable taking the value 1 if $Y_i = m$ and 0 otherwise.

Prove that maximizing the joint pmf is equivalent to minimizing the cross-entropy given in Equation (10.14). (HINT: take the logarithm and use properties of logs.)

# Applied Problems

## Applied Problem 1 (19 pts total)

Using the `Default` dataset, we wish to predict whether a customer will default on their debt.

```{r load ISLR2, message = FALSE, warning = FALSE}
library(ISLR2) # for Default dataset
library(Lahman)
library(tidyverse)
library(tidymodels)
library(vip)
library(xgboost) 
library(tidyclust)
library(GGally)
```

### Part a (Code: 1 pt)

Divide the Default dataset into a training set of 8000 observations (80% of the data) and a test set containing the remaining 2000 observations.

```{r 1a Default split}
set.seed(100)
default_split <- initial_split(Default, prop = 0.80)
default_train_tidy <- training(default_split)
default_test_tidy <- testing(default_split)
```

### Part b (Code: 3 pts; Explanation: 2 pts)

Fit a k-means clustering model on the training set using the three predictors (`student`, `balance`, and `income`). Either pick a reasonably small (3-4) number of clusters or tune the number of clusters.

Describe the general characteristics of each of the clusters identified. Supplement your description by producing a scatterplot of income and balance showing each cluster in a different color.

```{r set up kmeans}
km_model <- k_means(num_clusters = 4) %>%
  set_args(nstart = 20)

km_recipe_default <- recipe(~ student + balance + income, data = default_train_tidy) %>%
  step_dummy(student) %>%
  step_normalize(all_numeric_predictors()) 

km_wflow_default <- workflow() %>%
  add_model(km_model) %>%
  add_recipe(km_recipe_default)
```

```{r finalize kmeans workflow}
km_default_4clusters <- km_wflow_default %>% 
  finalize_workflow_tidyclust(parameters = list(num_clusters = 4))
```

```{r fit 4 cluster model}
set.seed(437) 
# always reset the seed before you re-fit, just in case something weird happens

km_default_fit <- km_default_4clusters %>%
  fit(data = default_train_tidy)
```

```{r ggpairs}
default_clu <- bind_cols(
  default_train_tidy,
  km_default_fit %>% extract_cluster_assignment()
)
ggpairs(default_clu, columns = c("student", "balance", "income"),
        aes(color = .cluster))

```

### Part b (Code: 3 pts)

Fit a random forest model (using either `randomForest` or `ranger`) on the training set. Use cross-validation to determine whether `mtry` should be set to 1, 2, or 3. 

`mtry` should be set to 1 based on mean_log_loss.

```{r rfC-tidy model}
rfC_model <- rand_forest(mode = "classification", engine = "ranger") %>%
  set_args(seed = 395,
           importance = "permutation",
           mtry = tune()
  )

rfC_recipe <- recipe(
  default ~ student + balance + income,
  data = default_train_tidy
)


rfC_wflow <- workflow() %>%
  add_model(rfC_model) %>%
  add_recipe(rfC_recipe) 
```

```{r tune model kfold rfC}
set.seed(1332)
default_kfold <- vfold_cv(default_train_tidy, v = 5, repeats = 3) 

# I'm sure there's a better way, but this works
n_predictorsC <- sum(rfC_recipe$var_info$role == "predictor")
manual_gridC <- expand.grid(mtry = seq(1, n_predictorsC))



rfC_tune1 <- tune_grid(rfC_model, 
                      rfC_recipe, 
                      resamples = default_kfold, 
                      metrics = metric_set(accuracy, mn_log_loss),
                      grid = manual_gridC)
```

```{r tune model kfold rf2C}
rfC_tune1 %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  ggplot(mapping = aes(x = mtry, y = mean)) + geom_point() + geom_line()

rfC_tune1 %>%
  collect_metrics() %>%
  filter(.metric == "mn_log_loss") %>%
  ggplot(mapping = aes(x = mtry, y = mean)) + geom_point() + geom_line()
```

```{r select best rfC}
rfC_best <- select_best(
  rfC_tune1,
  metric = "mn_log_loss",
  mtry
)
```

```{r fit rfC-tidy model}
rfC_wflow_final <- finalize_workflow(rfC_wflow, parameters = rfC_best) 

rfC_fit <- fit(rfC_wflow_final, data = default_train_tidy)
rfC_fit
```

```{r rfC get OOB Brier Score}
rfC_engine <- rfC_fit %>% extract_fit_engine()
rfC_fit %>% extract_fit_engine() %>% pluck("prediction.error")
```
-> The clusters are split up into 2 clusters for students and 2 clusters for non-students, with a few observations in both groups. For students, we have the salmon group and the green group. The red group has a high average balance and a high income. The green group has a low average balance but a high income. For non-students, we have the purple and blue group. The purple group has high average balance, but low income. Finally, the blue group has low average balance and low income.


### Part c (Code: 3 pts)

Fit a gradient-boosted trees model on the training set (using either `gbm` or `xgboost`). Use a learning rate of $\lambda = 0.01$ and an interaction depth of $d = 2$. You can either tune the number of trees or use a reasonably large number.

```{r xgboost R p1}
xgboostC_model_de <- boost_tree(mode = "classification", engine = "xgboost",
                            trees = tune(), tree_depth = 2,
                            learn_rate = 0.01)

xgboostC_recipe_de <- recipe(
  default ~ student +  balance + income,
  data = default_train_tidy
) %>%
  step_dummy(all_nominal_predictors())

xgboostC_wflow_de <- workflow() %>%
  add_model(xgboostC_model_de) %>%
  add_recipe(xgboostC_recipe_de)
```

```{r tune parameters xgboostR p2}
set.seed(100)
default_kfold <- vfold_cv(default_train_tidy, v = 5, repeats = 3) 

manual_gridC <- expand.grid(trees = seq(from=500,to=2000,by=100))

xgboostC_tune_de <- tune_grid(xgboostC_model_de, 
                      xgboostC_recipe_de, 
                      resamples = default_kfold,
                      metrics = metric_set(accuracy, mn_log_loss),
                      grid = manual_gridC)
```

```{r tune model kfold xgboostC p3}
xgboostC_tune_de %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean)) # best accuracy

xgboostC_tune_de %>%
  collect_metrics() %>%
  filter(.metric == "mn_log_loss") %>%
  arrange(mean)
```

```{r select best xgboostC p4}
xgboostC_best_de <- select_by_one_std_err(
  xgboostC_tune_de,
  metric = "mn_log_loss",
 trees
)
```

```{r fit xgboostC-tidy model p5}
xgboostC_wflow_final_de <- finalize_workflow(xgboostC_wflow_de, parameters = xgboostC_best_de) 

xgboostC_fit_de <- fit(xgboostC_wflow_final_de, data = default_train_tidy)
xgboostC_fit_de
```

### Part d (Code: 3 pts)

Fit a neural network on the training set. Either follow the instructions in Textbook Exercise 10.10.7 or adapt the example code in the "Single Hidden Layer Neural Network" activity.

```{r nnnC}
neuralnetC_model <- mlp(mode = "classification", engine = "keras",
                        hidden_units = tune(),
                        dropout = tune(),
                        epochs = 25,
                        activation = "relu") %>%
  set_args(seeds = c(1, 2, 3)) # we need to set 3 seeds 
# let's tune the dropout parameter instead

neuralnetC_recipe <- recipe(
  default ~ student + balance + income,
  data = default_train_tidy
) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())
# no nominal predictors here so won't do anything

neuralnetC_wflow <- workflow() %>%
  add_model(neuralnetC_model) %>%
  add_recipe(neuralnetC_recipe)
```

```{r check defaults nnC}
extract_parameter_set_dials(neuralnetC_model) %>%
  pull("object")
```

```{r tune parameters nnC}
set.seed(1332)
default_kfold <- vfold_cv(default_train_tidy, v = 5, repeats = 1) 


neuralnetC_tune <- tune_grid(neuralnetC_model, 
                      neuralnetC_recipe, 
                      resamples = default_kfold, 
                      metrics = metric_set(mn_log_loss),
                      grid = grid_regular(hidden_units(range = c(16,32)),
                                          dropout(range = c(0, 0.1)),
                                          levels = 2)
)
```


```{r select best nnC}
collect_metrics(neuralnetC_tune)

neuralnetC_best <- select_by_one_std_err(
  neuralnetC_tune,
  metric = "mn_log_loss",
  hidden_units, desc(dropout)
)
neuralnetC_best

neuralnetC_wflow_final <- finalize_workflow(neuralnetC_wflow, 
                                            parameters = neuralnetC_best) 

```

```{r fit nnC}
neuralnetC_fit <- fit(neuralnetC_wflow_final, data = default_train_tidy)
neuralnetC_fit
```


### Part e (Code: 2 pts; Explanation: 2 pts)

For each of the models in parts (b)-(d), predict on the test set and obtain the confusion matrix. Which model is making the best predictions? Why?

All the models have about the same accuracy, but by MCC, the gradient-boosted trees model is making the best predictions. 

```{r plot everything 3 clusters}
predictions3 <- augment(km_default_fit, new_data = default_test_tidy)
names(predictions3)
ggpairs(predictions3, columns = c("student", "balance", "income"),
         aes(color = .pred_cluster))


assignments3 <- bind_cols(
  default_train_tidy,
  km_default_fit %>% extract_cluster_assignment())

all_clusters3 <- bind_rows(
  assignments3,
  predictions3 %>% rename(.cluster = .pred_cluster) # rename cluster variable name
)
ggpairs(all_clusters3, columns = c("student", "balance", "income"),
        aes(color = .cluster))
```

```{r augment baggingR fit part b2}
predictions_rfc <- broom::augment(rfC_fit, new_data = default_test_tidy)
#names(predictions_rfc)

predictions_rfc %>% dplyr::select(
  default, student, balance, income, .pred_class
)

#mn_log_loss(predictions_rfc, truth = default, estimate = .pred_class)
#accuracy(predictions_rfc, truth = default, estimate = .pred_class)

rfc_conf_mat <- conf_mat(predictions_rfc,
                            truth = default,
                            estimate = .pred_class
                            )

rfc_conf_mat

summary(rfc_conf_mat)
```

```{r fit and predict xgboostC part c}
xgboostC_predict <- augment(xgboostC_fit_de, new_data = default_test_tidy)
#names(xgboostC_predict)
xgboostC_predict %>%
  dplyr::select(default, student,balance,income, .pred_class, .pred_No, .pred_Yes)

#accuracy(xgboostC_predict, truth = default, estimate = .pred_class)

xgboost_conf_mat <- conf_mat(xgboostC_predict,
                            truth = default,
                            estimate = .pred_class
                            )

xgboost_conf_mat

summary(xgboost_conf_mat)
```

```{r augment neuralnetC fit part d}
predictions_neuralnetC <- broom::augment(neuralnetC_fit, new_data = default_test_tidy)
#names(predictions_neuralnetC)

predictions_neuralnetC %>% dplyr::select(
 default, student,balance, income, .pred_class, .pred_No
)

nnc_conf_mat <- conf_mat(predictions_neuralnetC,
                            truth = default,
                            estimate = .pred_class
                            )

nnc_conf_mat

summary(nnc_conf_mat)
```
