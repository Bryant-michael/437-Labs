---
title: 'Lab Assignment #5'
author: "Vanessa Avilez, Michael Bryant, Phuong Traceyle"
date: "Due March 8, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce more advanced regression strategies that were probably not covered in Math 338.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this lab, we will be working with four datasets. Three (`Boston`, `Carseats`, and `Wage`) are contained in the `ISLR2` package. Information about these datasets can be found by searching R help for them.

The fourth dataset, `RateMyProfessor`, needs to be downloaded from Canvas. This dataset contains the overall average rating from <https://www.ratemyprofessors.com/> for over 22,000 professors, as collected by [Murray et al. (2020)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0233515). A data dictionary for the dataset can be found at <https://github.com/murrayds/aa_rmp/tree/master/data> (note that I removed a bunch of variables so that you're downloading a 2 MB dataset instead of a much larger one).


```{r libraries and data, message = FALSE, warning = FALSE, eval = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(broom) # See Problem 3b

RateMyProfessor <- read.csv("RateMyProfessor.csv")
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Indicator Variables

## Part a (Code: 0.5 pts)

Run the code in ISLR Lab 3.6.6. Put each chunk from the textbook in its own chunk.

```{r 3.6.6 p1}
head(Carseats)
```

```{r 3.6.6 p2}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age,
data = Carseats)
summary(lm.fit)
```

```{r 3.6.6 p3}
attach(Carseats)
contrasts(ShelveLoc)
```

## Part b (Explanation: 1 pt)

Interpret the slope estimate corresponding to `ShelveLocGood` in the model fit in part (a).

-> When a sale increases by 1 unit, ShelveLocGood increases by 4.85.

## Part c (Code: 1 pt; Explanation: 1.5 pts)

Using the RateMyProfessor dataset, fit a linear model predicting the overall rating of a professor (`overall`) from the difficulty rating (`difficulty`), chili pepper rating (`hotness`), and rank (`rank`). What are the reference levels for each categorical variable? How do you know?

```{r linear model for RMP}
lm1 <- lm(overall ~ difficulty + hotness + rank, data = RateMyProfessor)
summary(lm1)
```

-> The reference level for hotness is cold because the summary only shows hot. The reference level for rank is Assistant Professor because the summary only shows Associate Professor and Professor.

## Part d (Explanation: 1.5 pts)

Holding difficulty constant, which of the following instructors would be predicted to have the highest overall rating? Which would be predicted to have the lowest overall rating? Explain your reasoning.
```{r 1d, eval = FALSE}
overall = -0.5878*d + 0.6364*h + -0.0504*ap + -0.0474*p + 5.301
```

* Attractive Assistant Professor
  -> 5.9374
* Attractive Associate Professor
  -> 5.887
* Attractive Professor
  -> 5.89
* Less-attractive Assistant Professor
  -> 5.301
* Less-attractive Associate Professor
  -> 5.2506
* Less-attractive Professor
  -> 5.2536
  
We predict that an attractive assistant professor would have the highest overall rating. We also predict that a less-attractive associate professor would have the lowest overall rating. We obtained these results from using the least square regression equation obtained from the summary of lm1.
  
# Problem 2: Interaction Terms

## Part a (Code: 0.5 pts)

Run the single line of code in ISLR Lab 3.6.4. 

```{r ISLR lab 3.6.4}
summary(lm(medv~lstat*age, data = Boston))
```


## Part b (Explanation: 2 pts)

Notice that `age` is a significant predictor of `medv` in the model without the interaction term (from ISLR Lab 3.6.3 on Lab 4), but it is no longer a significant predictor of `medv` once we add in the interaction term. The p-value is huge (0.971!). What do you think is happening here? Are we okay to remove the `age` variable from the model with the interaction term? Why or why not?

The lstat is affecting the slope of age in the model so the p-value is bigger in this model. In this model, age is affected by the estimate and a 0.004 increase in lstat, because we are not holding it constant.
We are not okay to remove the `age` variable because we need to include the main effect of each variable even though `age` is not significant in the model.  

## Part c (Code: 1 pt; Explanation: 1.5 pts)

Create a new dataset, `associates`, by `filter`ing the `RateMyProfessor` dataset to include only the Associate Professors.

```{r subset RateMyProfessor}
associates <- RateMyProfessor %>% filter(rank %in% 'Associate Professor')
```


Next, complete this code chunk to create a graph of overall rating vs. difficulty rating for the associate professors, with "hot" professors shown in red and "cold" professors shown in blue. Remember to delete `eval = FALSE` once you get the code to run!

```{r ggplot associates}
ggplot(associates, aes(difficulty, overall, color = hotness)) + # add appropriate arguments here or in the next line
  geom_point(alpha = 0.25) +
  # if you added arguments to geom_point, add the same arguments to geom_smooth
  # otherwise don't touch these lines - they will run fine
  geom_smooth(method = "lm", se = FALSE) + # regression line, no confidence bands
  scale_color_manual(name = "Chili Pepper Rating",  # make legend nice
                     labels = c(hot = "Attractive", cold = "Less-attractive"),
                     values = c(hot = "red", cold = "blue")) 
```

How does the difficulty of the professor modify the relationship between attractiveness and overall rating?
The more difficult that students find a professor, the students will give them a lower overall rating but attractive professors generally have a higher overall over less-attractive professors.

## Part d (Code: 1 pt; Computation and Explanation: 2 pts)

Using the `RateMyProfessor` dataset, fit a linear model predicting overall rating from the difficulty rating (`difficulty`), chili pepper rating (`hotness`), rank (`rank`), and an interaction term between `difficulty` and `hotness`.

```{r lm of rate my professor, eval = FALSE}
#lm.interaction <- lm(salary ~ HR + BB + HR:BB, data = nlwest)
lm.int <- lm(overall~ difficulty + hotness + rank + difficulty:hotness, data = RateMyProfessor)
summary(lm.int)
```

Using your results, write out the least-squares regression equation predicting overall rating from difficulty for an attractive associate professor. Also, write out the least-squares regression equation predicting overall rating from difficulty for a less-attractive associate professor. Explain how you obtained each equation.
-> overall = 5.4709 - 0.6409(difficulty) - 0.2524(hotness = hot) - 0.0489(rank = Associate Professor) - 0.0448(rank = Professor) + 0.2971(difficulty)(hotness = hot)
when hotness = hot = 1 and rank = associate = 1, overall = 5.4709 - 0.6409(difficulty) - 0.2524(1) - 0.0489(1) - 0.0448(0) + 0.2971(difficulty)(1) = 5.1696 - 0.3438(difficulty). 

when hotness = cold = 0 and rank = associate = 1, overall = 5.4709 - 0.6409(difficulty) - 0.2524(0) - 0.0489(1) - 0.0448(0) + 0.2971(difficulty)(0) = 5.422 - 0.6409(difficulty)

Do your equations support your conclusions from part (c)? Explain why or why not.
Yes these equations support my claim from part (c). If you consider a difficulty = 4.5, then the overall for an attractive associate professor is 3.6225 while the overall for a less-attractive associate professor is 2.53795. so the more difficult a less attractive associate professor is, the less their overall is comapred to an attrctive associate professor.  

# Problem 3: Regression with Nonlinear Transformations of the Predictors

## Part a (Code: 0.5 pts)

Run the first four code chunks in ISLR Lab 7.8.1 (up through the point where `fit2b` is created). Put each chunk from the textbook in its own chunk.

```{r 7.8}
library(ISLR2)
attach(Wage)
```

```{r 7.8.1 p1}
fit <- lm(wage ~ poly(age , 4), data = Wage)
coef(summary(fit))
```

```{r 7.8.1 p2}
fit2 <- lm(wage ~ poly(age , 4, raw = T), data = Wage)
coef(summary(fit2))
```

```{r 7.8.1 p3}
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4),
data = Wage)
coef(fit2a)
```

```{r 7.8.1 p4}
fit2b <- lm(wage ~ cbind(age , age^2, age^3, age^4),
data = Wage)
```

## Part b (Code: 1 pt)

In the code chunk below, create a data frame with a single variable, `age`, ranging from 18 to 80, then use the `augment` function (in the `broom` package) to obtain the predicted wage, standard error of the mean wage, and the lower and upper bounds of a 95% confidence interval for the population mean wage at each age. (You can use any of `fit`, `fit2`, `fit2a`, or `fit2b` - they should all give the same predictions.)

What is the 95% confidence interval for the population mean wage of 25-year-olds? 50-year-olds?

```{r use augment to get predictions out}

```

