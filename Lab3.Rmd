---
title: 'Lab Assignment #3'
author: "Vanessa Avilez, Michael Bryant, Phuong Traceyle"
date: "Due February 15, 2023"
output:
  html_document:
    df_print: paged
---

# Instructions

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

There are two purposes to this lab. First, you will get comfortable with the family-wise error rate and false discovery rate. Then, we will get more practice with coding a nonparametric bootstrap estimate of standard error.

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Family-Wise Error Rate

## Part a (Code: 1 pt; Computation and Explanation: 0.5 pts)

Run the code in ISLR Labs 13.6.1 and 13.6.2. Put each chunk from the textbook in its own chunk.

```{r ISLR 6.1 p1}
set.seed(6)
x <- matrix(rnorm(10*100), 10, 100)
x [,1:50] <- x[, 1:50]+0.5
```

```{r ISLR 6.1 p2}
t.test(x[,1],mu = 0)
```

```{r ISLR 6.1 p3}
p.values <- rep(0,100)
for (i in 1:100) {
  p.values[i] <- t.test(x[,i], mu = 0)$p.value
}
decision <- rep("do not reject H0", 100)
decision[p.values <= .05] <- "reject H0"
```

```{r ISLR 6.1 p4}
table(decision ,
  c( rep ("H0 is False ", 50), rep ("H0 is True ", 50))
)
```

```{r ISLR 6.1 p5}
x <- matrix (rnorm (10 * 100), 10, 100)
x[, 1:50] <- x[, 1:50] + 1
for (i in 1:100) {
  p.values[i] <- t.test(x[, i], mu = 0)$p.value 
  }
decision <- rep ("Do not reject H0", 100)
decision[p.values <= .05] <- " Reject H0"
table (decision ,
c( rep ("H0 is False ", 50), rep ("H0 is True ", 50))
)
```


Using the output of Lab 13.6.1, estimate the power of this test to detect each of the two alternative hypotheses $\mu = 0.5$ and $\mu = 1$.

```{r 6.1 mu 0.5}
#power = # (reject H0 & H0 false)/ # H0 false
x <- matrix (rnorm (10 * 100), 10, 100)
x[, 1:50] <- x[, 1:50] + 1
for (i in 1:100) {
  p.values[i] <- t.test(x[, i], mu = 0.5)$p.value 
  }
decision <- rep ("Do not reject H0", 100)
decision[p.values <= .05] <- " Reject H0"
table (decision ,
c( rep ("H0 is False ", 50), rep ("H0 is True ", 50))
)

pow_5 = 17 / (17+33)
pow_5
```

```{r 6.1 mu 1}
#power = # (reject H0 & H0 false)/ # H0 false
x <- matrix (rnorm (10 * 100), 10, 100)
x[, 1:50] <- x[, 1:50] + 1
for (i in 1:100) {
  p.values[i] <- t.test(x[, i], mu = 1)$p.value 
  }
decision <- rep ("Do not reject H0", 100)
decision[p.values <= .05] <- " Reject H0"
table (decision ,
c( rep ("H0 is False ", 50), rep ("H0 is True ", 50))
)

pow_1 = 2 / 50
pow_1
```
Start of 13.6.2

```{r 13.6.2 p1}
m <- 1:500
fwe1 <- 1 - (1 - 0.05)^m
fwe2 <- 1 - (1 - 0.01)^m
fwe3 <- 1 - (1 - 0.001)^m
```

```{r 13.6.2 p2}
par(mfrow = c(1, 1))
plot(m, fwe1, type = "l", log = "x", ylim = c(0, 1), col = 2, 
     ylab = "Family - Wise Error Rate",
     xlab = "Number of Hypotheses")
lines(m, fwe2, col = 4)
lines(m, fwe3, col = 3)
abline(h = 0.05, lty = 2)
```

```{r 13.6.2 p3}
library(ISLR2)
fund.mini <- Fund[, 1:5]
t.test(fund.mini[, 1], mu = 0)
fund.pvalue <- rep(0, 5)
for (i in 1:5){
  fund.pvalue[i] <- t.test(fund.mini[, i], mu = 0)$p.value
}
fund.pvalue
```

```{r 13.6.2 p4}
p.adjust(fund.pvalue, method = "bonferroni")
pmin(fund.pvalue * 5, 1)
```

```{r 13.6.2 p5}
p.adjust(fund.pvalue, method = "holm")
```

```{r 13.6.2 p6}
apply(fund.mini, 2, mean)
```

```{r 13.6.2 p7}
t.test(fund.mini[, 1], fund.mini[, 2], paired = T)
```

```{r 13.6.2 p8}
returns <- as.vector(as.matrix(fund.mini))
manager <- rep(c("1", "2", "3", "4", "5"), rep(50, 5))
a1 <- aov(returns ~ manager)
TukeyHSD(x = a1)
```

```{r 13.6.2 p9}
plot(TukeyHSD(x = a1))
```

## Part b (Explanation: 1 pt)

What does the `p.adjust` function do exactly when `method = bonferroni`? What about when `method = holm`? Why does it make more sense for R to adjust the p-values rather than the significance level when controlling the FWER?

Under Bonferroni, the p-values are multiplied by the number of comparisons, while controlling FWER by testing each hypothesis at significance level alpha/m. For Holm, there are less conservative corrections and the significance level depends on the values of all m of the p-values. In running this, we only use the p-values and the length of p-values. Computations regarding adjusted p-value is easier with given p-values and length of p-values. 

## Part c (Explanation: 1 pt)

Consider the second-to-last chunk in ISLR Lab 13.6.2 (the one using `TukeyHSD`). Is it appropriate to do a one-way ANOVA with this data? Explain why or why not. (You may want to produce some graphs and/or numerical summaries to support your answer.)

-> It is not appropriate to do a one-way ANOVA with this data because the data is not independent based on the numerical summaries of their performance/behavior.

# Problem 2: False Discovery Rate

## Part a (Code: 0.5 pts)

Run the code in ISLR Lab 13.6.3. Put each chunk from the textbook in its own chunk.

```{r ISLR 6.3 p1}
fund.pvalues <- rep(0,2000)
for (i in 1:2000) {
  fund.pvalues[i] <- t.test (Fund[, i], mu = 0)$p.value
}
```

```{r ISLR 6.3 p2}
q.values.BH <- p.adjust(fund.pvalues, method = "BH")
q.values.BH[1:10]
```

```{r ISLR 6.3 P3}
sum(q.values.BH <= .1)
```

```{r ISLR 6.3 P4}
sum(fund.pvalues <= (0.1 /2000))
```

```{r ISLR 6.3 P5}
ps <- sort(fund.pvalues)
m <- length(fund.pvalues)
q <- 0.1
wh.ps <- which(ps < q * (1:m) / m)
if (length(wh.ps) > 0 ) {
  wh <- 1:max(wh.ps)
} else {
  wh <- numeric(0)
}
```


```{r ISLR 6.3 P6}
plot (ps , log = "xy", ylim = c(4e-6, 1), ylab = "P- Value ",
xlab = " Index ", main = "")
points (wh, ps[wh], col = 4)
abline (a = 0, b = (q / m), col = 2, untf = TRUE)
abline (h = 0.1 / 2000, col = 3)
```



## Part b (Code: 1 pt)

Finish writing the `FDR_plot` function in the code chunk below. This function should take two arguments: `p.values`, a vector of p-values, and `q`, the desired False Discovery Rate, and produce a graph like those in Figure 13.6. (Assume that on the graphs, $q$ and $\alpha$ are set to the same value.)

```{r FDR plot}
FDR_plot <- function(p.values, q){
  
  # Copy code from the last two chunks of ISLR Lab 13.6.3, but replace variable names and hard-coded values as necessary
  
  ps <- sort (p.values)
  m <- length (p.values)
  wh.ps <- which (ps < q * (1:m) / m)
  if ( length (wh.ps) >0) {
  wh <- 1: max (wh.ps)
  } else {
  wh <- numeric (0)
  }
  
  plot (ps , log = "xy", ylim = c(4e-6, 1), ylab = "P- Value ",
xlab = " Index ", main = "")
  points (wh, ps[wh], col = 4)
  abline (a = 0, b = (q / m), col = 2, untf = TRUE)
  abline (h = 0.1 / 2000, col = 3)
  invisible(wh.ps) # invisibly return the significant p-values
}
```

Test your function. First, attempt to duplicate the left and right panels of Figure 13.6. 

```{r ISLR 13.6 Left and Right}
par(mfrow = c(1,2))
FDR_plot(fund.pvalues,0.05)
FDR_plot(fund.pvalues,0.3)
```

Then, test your function on the simulated dataset in the chunk below, using $q = 0.1$.

```{r sim p-values}
set.seed(12)
sim_pvalues <- runif(1000, min = 1e-5, max = 0.05001)
FDR_plot(sim_pvalues,.1)
# Now put a line of code running your function on this set of "significant" p-values
```

# Problem 3: Simulation Study of FWER and FDR

This problem is adapted from ISLR Chapter 13, Exercise 8.

## Part a (Code: 1 pt)

Using the code in Exercise 13.7.8, create a 20 x 100 matrix where each column represents 20 random numbers from $N(0, 1)$. 

```{r Code 13.7.8}
set.seed(1)
n <- 20
m <- 100
X <- matrix ( rnorm (n * m), ncol = m)

```

Then, run a t-test on each column of the matrix testing $H_0: \mu = 0$ against $H_a: \mu \neq 0$. We are going to use the `apply` function to do this rather than adapt the `for` loop from the ISLR Labs. (Recall from the class activities that the `apply` function applies a single function to each row (`MARGIN = 1`) or column (`MARGIN = 2`) of a matrix.)

Don't forget to delete the `eval = FALSE` after you've fixed this code chunk to run properly!

```{r t-test-apply}
t_test_0 <- function(x){
  # x: a vector of data
  
  p_value <- t.test(x, mu = 0)$p.value# write a line of code that extracts the p-value from the t-test we want to do
    
  return(p_value)

}

p_values <- apply(X, 2, t_test_0)
```

Plot a histogram of the p-values obtained.

```{r 13.7.8a histogram}
hist(p_values)
```

## Part b (Code: 1 pt, Explanation: 0.5 pts)

Without any adjustment for multiple hypothesis tests, how many null hypotheses would be rejected at $\alpha = 0.05$? We can take advantage of the fact that R implicitly converts *logical* (TRUE/FALSE) variables to *numerical* variables.

```{r 13.7.8b}
alpha <- 0.05
sum(p_values <= alpha)
```

Obtain the adjusted p-values using the Holm step-down procedure. How many null hypotheses would be rejected if we control the FWER at 0.05?

```{r 13.7.8c holm}
p.values.H <- p.adjust(p_values, method = "holm")
p.values.H
hist(p.values.H)
```

None of the null hypothesis can reject the null at 0.05 for Holm.

Obtain the adjusted p-values using the Benjamini-Hochberg procedure. How many null hypotheses would be rejected if we control the FDR at 0.05?

```{r 13.7.8d bh}
p.values.BH <- p.adjust(p_values, method = "BH")
p.values.BH
hist(p.values.BH)
```
None of the null hypothesis can reject the null at 0.05 for Benjamini-Hochberg

## Part c (Code: 1 pt)
Create a new matrix, `X2`, that is exactly the same as X, except that the first 25 fund managers do actually have a slight long-term return of $+1\%$ (i.e., in rows 1-25 $\mu = 1$ not 0.01). Conduct your 100 t-tests and plot a histogram of the new p-values.

```{r create X2}
X2 <- matrix(c(rnorm(25*20,mean = 1),rnorm(75*20, mean = 0)),ncol = 100)
```

```{r t-test-apply redux}
# don't rewrite the function, just apply it to the new dataset!
t_test_0(X2)
p_values2 <- apply(X2, 2, t_test_0)
```

```{r 13.7.8 histogram redux}
hist(p_values2)
```

## Part d (Code: 1 pt, Explanation: 1 pt)

Without any adjustment for multiple hypothesis tests, how many of the 75 true null hypotheses would be rejected at $\alpha = 0.05$? How many of the 25 false null hypotheses would be rejected?

All the 75 true null would not be rejected and all the 25 null hypotheses would be rejected.

```{r 13.7.8b redux}
reject.falseH0 <- sum(p_values2[1:25] <= alpha)
reject.trueH0 <- sum(p_values2[26:100] <= alpha)
c(true = reject.trueH0, false = reject.falseH0)
```

Obtain the adjusted p-values using the Holm step-down procedure. How many null hypotheses of each type would be rejected if we control the FWER at 0.05?

```{r 13.7.8c holm redux}
p.values2.H <- p.adjust(p_values2, method = "holm")
p.values2.H
hist(p.values2.H)
```

Obtain the adjusted p-values using the Benjamini-Hochberg procedure. How many null hypotheses of each type would be rejected if we control the FDR at 0.05?

```{r 13.7.8d bh redux}
p.values2.BH <- p.adjust(p_values2, method = "BH")
p.values2.BH
hist(p.values2.BH)

```

Compare your results from the three procedures. Why is it important to consider the situation where *some* null hypotheses are true, when evaluating the performance of the different procedures?

-> It is important to consider the situations where some null hypothesis are true because it affects the Type 1 and Type 2 error rates that occur when we adjust the p-values to control the FWER and the FDR. 
