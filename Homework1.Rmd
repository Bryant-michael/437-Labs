---
title: 'Homework Assignment #1'
author: "Vanessa Avilez, Michael Bryant, Phuong Traceyle"
date: "Due February 10, 2023"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Instructions

You should submit either two or three files:

1. You should write your solutions to the Simulation and Applied Problems in this R Markdown file and submit the (.Rmd) file.
2. You should knit the final solution file to pdf and submit the pdf. If you are having trouble getting code chunks to run, add `eval = FALSE` to the chunks that do not run. If you are having trouble getting R Studio to play nice with your LaTeX distribution, I will begrudgingly accept an HTML file instead. 
3. Solutions to the Key Terms and Conceptual Problems can be submitted in a separate Word or pdf file or included in the same files as your solutions to the Simulation and Applied Problems.

This homework assignment is worth a total of **40 points**.

```{r libraries}
library(ISLR2)
library(dplyr)

```


# Key Terms (5 pts)

Read Chapter 2 of Introduction to Statistical Learning, Second Edition. Based on your reading, answer the following questions.

# Phuong 1,4,7
# Vanessa 2,5,8,10
# Michael 3,6,9

1. What is the difference between an *input variable* and an *output variable* in a model? Provide synonyms for each term.
-> Input variable goes into the function, while output varible is what comes out of the function. Synonyms for input variable are predictor and independent variable. Synonyms for output variable are response and dependent variable.

2. What is the difference between *reducible error* and *irreducible error*? Give an example (other than those given in the book) of a situation in which the irreducible error is greater than zero.
-> A reducible error can be improved to estimate the response variable using statistical techniques where as a irreducible cannot be improved or change but is always greater than zero. An example of an irreducible error is the risk of an inaccurate measure of an off day data when calculating who is the best pitcher in the MLB. 

3. Generally, what types of questions are answered using *inference* and what types are answered using *prediction*? Is it possible to use the same model for both inference and prediction?
-> Prediction is generally used when the question calls for finding new data based on previous data. Inference is used when a question calls for the relationship between the variables and the response. It is possible for the same model to be used in both inference and prediction. For example, linear regression is poor at predicting, but it can be used for it while also being useful for performing inference on the variables.

4. Generally, what types of prediction questions are answered using *regression* methods and what types are answered using *classification* methods?
-> Logistic regression and linear discriminant analysis are used for classification methods. Regression analysis and regression models are used for regression methods.

5. What are the major advantages of using a *nonparametric* method over a *parametric* method? What are the disadvantages?
-> The advantage of using a non-parametric method is there are less assumptions that must be met so it is easier to use the same model to estimate different sets of data. The disadvantage of a non-parametric method is space, when using a parametric method, the only thing that must be shared and stored is typically an equation while a non-parametric method must be stored since it uses the original data to model but some data sets are extremely large and costly to store.

6. In prediction, we typically aim to minimize a *loss function* that more-or-less represents the total error in our predictions. Give one example each for regression and classification problems of a measure of model (in)accuracy.
-> One example of a regression measure of model accuracy is linear regression. One example of a classification measure of model accuracy is AOC.

7. Why do we only fit the model on a *training set*? What do we do with the rest of the data?
-> We only fit the model on a training set because we want to select the smallest training MSE. We can compute and minimize the rest of the data to find the small MSE, even though we are not really interested.

8. Generally, as a model becomes more complex, what happens to the *bias* of the model and why? What happens to the *variance* of the model and why?
-> As the model becomes more complex and less flexible, the bias of the model increases and  the variance of the model decreases because there is more data to use for the training data so there will be a different f to use to fit the statistical learning method.  

9. What is meant by the term *overfitting*? Explain this in terms of the bias-variance trade-off.
-> A model that is more complex and less flexible means that the bias of the model increases. This makes the model great at predicting the training data, but should new data be predicted, the error will be extremely large. This is overfitting.

10. Briefly explain how a *Bayes classifier* works.
-> A Bayes classifier takes each observation and assigned them to the most likely class, given its predictor values. The classifier will always choose the class which Pr(Y = j| X = x0) is largest. In a two-class problem where there are two possible response values, the Bayes classifier corresponds to predicting class one if Pr(Y =1|X = x0) > 0.5 and class two otherwise. The Bayes classifier's prediction is determined by the Bayes decision boundary. This produces the lowest possible test error rate, called the Bayes error rate. 


# Conceptual Problems
## Phuong

## Conceptual Problem 1 (4 pts)

Write me a brief (2-3 paragraphs) summary of what you learned in the P-Values and Power in-class activity about how the distribution of p-values (over very many tests) is affected by the validity/violation of test assumptions and the power of the test. Did anything surprise you or clarify a concept for you? Support your writing with a few graphs you produced in class (it is easiest to copy and re-run the relevant code chunks).
-> In Problem 1, we used the poisson distribution to complete the simulation. In Problem 2, the histograms look similar but have different z-score ranges in different directions. The histogram of the normal distribution has more z-scores in the negatives while the histogram of the poisson distribution has more z-scores in the positives. The histograms of the two-sided p-value have obvious differences. The normal distribution is more uniform across the p-values, while the poisson distribution has more variation with the frequency of each p-value. The empirical cdf's of both distributions have a similar shape, but the normal distribution is continuous while the poisson distribution is discrete. We would trust the p-value from a one-sample z-test, but not fully because while both histograms are uniform, they are still varied. In Problem 3, we saw how changing power affected the p-value when we rejected the null. In Problem 4, we saw how changing the power and significance level affected the p-value. 
  This simulation did clarify for us what a power does and how changing the power and significance level affect the p-value. In Problem 5, when alpha is low, the cdf the probabilities of getting a smaller p-value has a higher range. When alpha is high, there is less of a possibility of getting a smaller p-value. When alpha is very low, the probability of getting a smaller p-value increases. When alpha is very high, there is less of a a chance of getting a small p-value. In Problem 6, under adequately powered studies, the probability of 82% of the p-values to being smaller is generally higher under Ha compared to Ho. In underpowered studies, the probability under Ha gets generally lower. In severely underpowered studies, Ha and Ho have almost the same probabilities, even being lower than Ho with sufficiently low powers. In all cases, the probability under Ho is always the same.

```{r p-value cdf function}
pvalue_cdf <- function(alpha = 0.05, power = 0.80){
  
  z_alpha <- qnorm(alpha/2, mean = 0, sd = 1, lower.tail = TRUE)
  z_beta <- qnorm(power, mean = 0, sd = 1, lower.tail = FALSE) 
  # z_beta is an overestimate but difference is negligible at high power 
  delta_z <- abs(z_alpha + z_beta)
  
  graph_title <- paste0("CDF of the P-Value with ", 
                        round(alpha*100, floor(-log10(alpha) + 2)), 
                        "% Sig. Level and ", 
                        round(power*100, floor(-log10(power) + 2)), 
                        "% Power" )
  
  z <- seq(0.1, 5, by = 0.01)
  pvalue <- pnorm(z, 0, 1, lower.tail = FALSE)*2
  pz <- pnorm(z, delta_z, 1, lower.tail = FALSE)
  plot(pvalue, pz, type = "l", col = "blue", log = "x",
       xlab = "P-Value",
       ylab = "Probability of Getting a Smaller P-Value",
       ylim = c(0,1),
       main = graph_title)
  
  lines(pvalue, pvalue, lty = "dashed")
  legend("topleft",
         legend = c("Under H0", "Under Ha"),
         col = c("black", "blue"),
         lty = c("dashed", "solid"))
  
  pvalue_df <- data.frame(pvalue = pvalue,
                          psmaller = pz)
  
  invisible(pvalue_df)
}
```


In the chunk below, write a single line of R code to call this function with `alpha = 0.01` and `power = 0.90`. Briefly explain what the function does.

```{r first pvalue_cdf call}
pvalue_cdf(alpha = 0.01, power= 0.90)
```
```{r calling function with different alphas}
pvalue_cdf(alpha = 0.0001)
pvalue_cdf(alpha = 0.085)
pvalue_cdf(alpha = 0.03)
pvalue_cdf(alpha = 0.1)
pvalue_cdf(alpha = 0.69)
```
```{r calling function with different powers}
pvalue_cdf(power = .1)
pvalue_cdf(power = .5)
pvalue_cdf(power = .69)
pvalue_cdf(power = .82)
pvalue_cdf(power = .9999999999)
```

## Conceptual Problem 2 (3 pts) 

Textbook Exercise 2.4.4

a) Classification is useful for qualitative responses such as a person's marital status (predictors: married or not), spam filtering (predictors: spam or non-spam), and customer behavior (predictors: likely to purchase more items or not). The goals of a person's marital status and spam filtering are inferences because we can determine the results based on the predictors. However, the goal of customer behavior is prediction because there is a likelihood that customers are about to make more purchases.

b) Regression is useful for quantitative problems such as business companies (advertising spending as predictor and revenue as response), medical researchers (drug dosage as predictor and blood pressure as response), and agricultural scientists (fertilizer and water as predictors and crop yield as response). The goals of business companies, medical researchers, and agricultural scientists are prediction because the results depend on the amount of the predictors over time.

c) Cluster is useful for health insurance (cluster of household size, number of doctor visits per year, and average age of household members), retail marketing (cluster of family size and spending level), and streaming services (cluster to identify the amount of usage users).


## Conceptual Problem 3 (3 pts) 

Textbook Exercise 13.7.2

a) If P($H_j$ rejected) = alpha, then $A_j$ ~ BER($\alpha$).

b) If B = $\sum_{j=1} ^ {m} A_j$, which are independent and identically distributed, then the distribution is B ~ BINOM(m,$\alpha$).

c) The standard deviation is $\sqrt{m * \alpha *(1-\alpha)}$.


# Simulation Problems
# Vanessa 

## Simulation Problem 1 (Code: 4 pts; Explanation: 6 pts)

From the Parametric vs. Nonparametric Tests: Two-Sample Tests activity, copy to this homework your simulation code/results from the *Assumptions Violated, Ha True* section of each test as well as the results tables for all simulations (in the Class Results section). 

```{r code from assumptions violated ha true}
#t.test
#{r assumptions violated ha true}
pvalues <- numeric(length = 10000)

nG <- 25# fill in your value of nG
d = 0.5
for (i in 1:length(pvalues)){
  set.seed(i)  # notice that the seed changes every time inside the for loop
  # you could also set a single seed outside the for loop
  
  # Create the vectors x and y
  x <- rnorm(nG) # finish this line
  y <- c(rnorm(22,mean=d,sd = sqrt(0.19)),rnorm(3, mean = 3+d, sd = sqrt(0.19))) # finish this line

  # Perform the t-test and get the p-value
  pvalues[i] <- t.test(x = x, y = y, alternative= "two.sided")$p.value # finish this line
    
}

# histogram of the p-values under H0
hist(pvalues)

plot(ecdf(pvalues),
     xlab = "p-value",
     ylab = "F(p-value)",
     main = "Empirical CDF of the P-Value Under H0")

mean(pvalues <= 0.05)



```

```{r code from assumptions violated ha true M-W}
#Mann-Whitney
#{r assumptions violated ha true}
pvalues <- numeric(length = 10000)

nG <- 25# fill in your value of nG
d = 0.5
for (i in 1:length(pvalues)){
  set.seed(i)  # notice that the seed changes every time inside the for loop
  # you could also set a single seed outside the for loop
  
  # Create the vectors x and y
  x <- rnorm(nG) # finish this line
  y <- c(rnorm(22,mean=d,sd = sqrt(0.19)),rnorm(3, mean = 3+d, sd = sqrt(0.19))) # finish this line

  # Perform the t-test and get the p-value
  pvalues[i] <- wilcox.test(x = x, y = y, alternative= "two.sided")$p.value # finish this line
    
}

# histogram of the p-values under H0
hist(pvalues)

plot(ecdf(pvalues),
     xlab = "p-value",
     ylab = "F(p-value)",
     main = "Empirical CDF of the P-Value Under H0")

mean(pvalues <= 0.05)
```

Class Results

Our simulations estimated the following.

For the two sample t-test using $\alpha = 0.05$:

$$
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$n_G$ & t-Test Assumptions & Type I Error Rate & Power at $d = 0.2$ & Power at $d = 0.5$ & Power at $d = 0.8$\\
\hline
10 & Met & 0.05 & 6.95% & 18.18% & 38.3% \\
\hline
25 & Met & 0.05 & 10.32% & 40% & 79% \\
\hline
50 & Met & 0.05 & 15.92% & 68.12% & 98% \\
\hline
10 & Violated & 0 & 0.02% & 16.34% & 20.4% \\
\hline
25 & Violated & 0 & 0.06% & 89% & 93% \\
\hline
50 & Violated & 0 & 1.23% & 86.55% & 100% \\
\hline
\end{tabular}
$$

For the Mann-Whitney test using $\alpha = 0.05$:

$$
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$n_G$ & t-Test Assumptions & Type I Error Rate & Power at $d = 0.2$ & Power at $d = 0.5$ & Power at $d = 0.8$\\
\hline
10 & Met & 0.05 & 6.16% & 1.5% & 34.5% \\
\hline
25 & Met & 0.05 & 10.19% & 39% & 77% \\
\hline
50 & Met & 0.05 & 15.53% & 66.22% & 100% \\
\hline
10 & Violated & 0.02 & 6.65% & 42.44% & 86.5% \\
\hline
25 & Violated & 0.02 & 17.66% & 78% & 99.97% \\
\hline
50 & Violated & 0.02 & 41.39% & 99.86% & 100% \\
\hline
\end{tabular}
$$


Write a couple of paragraphs explaining the difference between parametric and nonparametric methods and explain why classic nonparametric methods (Mann-Whitney and Kruskal-Wallis) are a better choice than the corresponding parametric methods (two-sample t-test and one-way ANOVA) when the assumptions of the parametric method are clearly violated.

-> A parametric method uses an equation or distribution to model data. Parametric methods are easier to share. Parametric methods usually have more assumptions then non-parametric methods. non-parametric methods use the data to create a model. there is no equation used. It takes up more space since the original data set must be included to run the model. We would use a non-parametric method when assumptions are violated for a parametric methods or when there is a small data set. 
 

# Applied Problems
# Michael

## Applied Problem 1 (Code: 6 pts; Explanation: 3 pts)

Textbook Exercise 2.4.8 with the following changes:

* Use the `College` dataset already in the `ISLR2` package instead of doing parts (a) and (b).
* Replace the four lines of code in part (c.iv) with a single line that accomplishes the same thing, using the `mutate` and either `if_else` or `case_when` functions from the `dplyr` package.
```{r Applied 1}
# c

summary(College)
pairs(College[,1:10])

plot (Outstate ~ Private, data = College)


College <- College %>% mutate(Elite = as.factor(if_else(College$Top10perc > 50, "Yes","No")) )
summary(College$Elite)
plot (Outstate ~ Elite, data = College)

par(mfrow = c(2, 2))
hist(College$Books)
hist(College$Personal)
hist(College$PhD)
hist(College$S.F.Ratio)

hist(College$Terminal)
hist(College$Apps)
hist(College$Accept)
hist(College$Enroll)
hist(College$Top10perc)
hist(College$Top25perc)
hist(College$Grad.Rate)
hist(College$perc.alumni) 
hist(College$F.Undergrad)
hist(College$P.Undergrad)
hist(College$Expend)
```


* As part of your brief summary in part (c.vi), identify at least one data point that cannot possibly have been recorded correctly, and explain why.
Many colleges have around 500 books, roughly 80 percent faculty with PhD. While not visible in the histogram, one of the college's graduation rate is 118%, which is impossible.

## Applied Problem 2 (Code: 1 pt; Explanation: 2 pts)

Molitor (1989) hypothesized that children who watched violent film and television were more tolerant of violent "real-life" behavior. A sample of 42 children were randomly assigned to watch footage from either the 1984 Summer Olympics (non-violent) or the movie \emph{The Karate Kid} (violent). They were then told to watch (by video monitor) two younger children in the next room and get the research assistant if they "got into trouble" (the monitor actually showed a pre-recorded video of the children getting progressively more violent).

The file \emph{violence.csv} contains the time (in seconds) that each child stayed in the room. Longer stays are assumed to indicate more tolerance of violent behavior. Produce an appropriate graph showing the sample data and, based on your graph, explain why a two-sample t-test might not be the best idea.


```{r Applied 2}
violence <- read.csv("violence.csv")
par(mfrow=c(1,2))
plot(Time ~ as.factor(Video), data = violence)
```

One of the two-sample t-test assumptions includes normality, and for a box plot, if normality is present, the graph should be symmetric, which it is not.


## Applied Problem 3 (Code: 1 pt; Explanation: 2 pts)

Use the permutation test function you wrote in Lab 2 to determine whether the research hypothesis in the previous question was supported. Be sure to follow all steps of hypothesis testing, up to and including writing a conclusion that answers the research question in context.
```{r script to function}
permutation_t_test <- function(formula, data, alternative = "t",n_sim = 99){
  # remember to include any other arguments you initialized in Step 0
  # I recommend giving those arguments default values = what you set in Step 0
  set.seed(9034)
#data <- data.frame(group = rep(c("Group 1", "Group 2"), 50), y = rnorm(100)) # simulated data
#formula <- y ~ group
#alternative <- "t"
# Initialize and assign values to any other arguments you identified in Part a
#n_sim <- 10^2 -1
  # Copy your complete script of Steps 1-6 in Part d
  # Step 1: Run the t-test on the original dataset and obtain the observed t-statistic value

# Creating this permutation_df will allow us to ignore other variables in the data frame.
# This will make Step 4 much easier - see comments in Step 4a.
  permutation_df <- model.frame(formula = formula, data = data)

# We only care about the t-statistic, but we need to store it in a variable
  t_obs <- t.test(formula = formula, data = permutation_df, 
                  alternative = alternative, var.equal = TRUE)$stat

# Step 2: Create a vector to store the simulated t-statistics in
  t_perm <- numeric(n_sim)
  
# Step 3: Set a seed for reproducibility of the resampling
  set.seed(15)
  
# Step 4: For i in 1 to number of permutation resamples:
for (i in 1:n_sim){ # Complete the syntax
  
  # Step 4a re-randomize the response variable
  permutation_df[[1]] <- sample(permutation_df[[1]], size = length(permutation_df[[1]]), replace = FALSE)
  # Use the course notes as a template for doing this step
  # Note: Once we get to Part e and put this script inside a function environment,
  # using the actual name of the response variable requires much more advanced R.
  # Remember from Lab 1 that this is an alternative way to do the column indexing
  # and we set up permutation_df so that the first column is the response variable.
    
  # Step 4b Obtain the value of the test statistic
  t_perm[i] <- t.test(formula = formula, data = permutation_df, 
                  alternative = alternative, var.equal = TRUE)$stat
}

# Step 5 Obtain the p-value    
  # Use the course notes as a template for getting p_left and p_right
  t_all <- c(t_obs, t_perm)
  p_left <- sum(t_all <= t_obs)/(100)
  p_right <- sum(t_all >= t_obs)/(100)
  
  # Use the switch function to compute the correct p-value
  p_value <- dplyr::case_when(alternative == "g" ~ p_right,
                              alternative == "l" ~ p_left,
                              alternative == "t" ~ 2*min(p_left, p_right),
                              TRUE ~ NaN # output NaN if alternative is anything else
  )
  
# Step 6: Create a list containing all the components of the output and then output it
# I've started the list, but you may need to finish it
  results <- list(obs = t_obs,
                  sim = t_perm,
                  p_v = p_value)
  
  return(results) # Last line of the function returns our output
}
```

```{r Applied 3}
permutation_t_test(formula = Time ~ Video,data = violence, alternative = "g")
```

With a p-value of 0.09 > 0.05 alpha significance level, we fail to reject the null hypothesis that kids who watch violent media are more tolerant of real life violence.





