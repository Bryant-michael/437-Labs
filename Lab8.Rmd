---
title: 'Lab Assignment #8'
author: "Vanessa Avilez, Michael Bryant, Phuong Traceyle"
date: "Due April 5, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce model selection in regression.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(leaps)

madden17_QB <- readr::read_csv("madden17_QB.csv")
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Model Selection Using Cross-Validation

## Part a (Code: 1 pt)

Run the code in ISLR Labs 5.3.2 and 5.3.3. Put each chunk from the textbook in its own chunk.

```{r 5.3.2 p1}
glm.fit <- glm(mpg~horsepower, data = Auto)
coef(glm.fit)
```

```{r 5.3.2 p2}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
```

```{r 5.3.2 p3}
#library(boot) #included earlier
glm.fit <- glm(mpg~horsepower , data = Auto)
cv.err <- cv.glm (Auto , glm.fit)
cv.err$delta
```

```{r 5.3.2 p4}
cv.error <- rep (0, 10)
for (i in 1:10) {
  glm.fit <- glm (mpg ~ poly(horsepower , i), data = Auto)
  cv.error[i] <- cv.glm (Auto , glm.fit)$delta[1]
}
cv.error
```

```{r 5.3.3 p1}
set.seed(17)
cv.error.10 <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm (mpg ~ poly (horsepower , i), data = Auto)
  cv.error.10[i] <- cv.glm (Auto , glm.fit , K = 10)$delta[1]
}
cv.error.10
```

## Part b (Code: 3 pts; Explanation: 1 pt)

Using the Auto dataset and 5-fold cross-validation, determine which of these sets of predictors produces the best linear model for predicting mpg, and explain your reasoning:

* horsepower and displacement
* acceleration and displacement
* horsepower and acceleration
* horsepower, acceleration, and displacement

*Do not* use the `cv.glm` function. Instead, modify the code in the "Automated Model Selection" class activity to do the cross-validation. Either the "Base R" or "tidymodels" example is fine to follow.

### Step 1: Set up your folds

```{r set up k folds}
set.seed(17)
k <- 5
n <- nrow(mpg)
reorder_rows <- sample(n)
fold_numbers <- (reorder_rows %% k) + 1 # otherwise we get fold 0
```

### Step 2: Create a function for your accuracy metric

```{r MSE function}
model_MSE <- function(model, df, response){
  # model: a model object
  # df: a data frame on which we want to predict
  # response: a character vector giving the name of the response variable
  
  predictions <- predict(model, newdata = df)
  MSE <- mean((predictions - df[[response]])^2)
  return(MSE)
}
```

### Step 3: Create all the models of interest

```{r create all models}
models <- vector("list", length = 4)
models[[1]] <- lm(mpg ~ horsepower + displacement, data = Auto)
models[[2]] <- lm(mpg ~ acceleration + displacement, data = Auto)
models[[3]] <- lm(mpg ~ horsepower + acceleration, data = Auto)
models[[4]] <- lm(mpg ~ horsepower + displacement + acceleration, data = Auto)
```

### Step 4: Run the cross-validation

```{r model fits}
nmodels <- length(models)
cv_error <- matrix(0, nrow = k, ncol = nmodels)
# each row of cv_error represents a fold
# each column of cv_error represents a model

for (i in 1:k){
  fold_validation_rows <- which(fold_numbers == i)
  train_set <- Auto[-fold_validation_rows,]
  validation_set <- Auto[fold_validation_rows,]
  
  for(j in 1:nmodels){
    models[[j]] <- update(models[[j]], data = train_set)
    cv_error[i, j] <- model_MSE(models[[j]], df = validation_set, response = "mpg")
  }
}

```

### Step 5: Obtain estimates of MSE

```{r estimate MSE}
apply(cv_error, 2, mean)
```

```{r estimate RMSE}
cv_rmse <- sqrt(cv_error)
apply(cv_rmse, 2, mean)
```
Here model 2 has the lower cross-validated MSE and RMSE. It is 3.88 off the actual value. 

## Part c (Code: 2 pts)

For the model you selected in part (b), re-fit the model on the entire `Auto` dataset. Then, write a couple of lines of code to compute $C_p$ and $BIC$ for this model (as given in the book) without relying on the `AIC`/`BIC` functions or any functions in the `olsrr` package. Some hints:

* You can obtain RSS by creating an `aov` object and running the code `summary(aov_object)[[1]]`, then finding the appropriate way to subset the resulting matrix.
* You can obtain $\hat{\sigma}$ for a model by running `summary(full_model)$sigma`. You may assume that the full model is the one with all three predictors.

```{r 1c refit}
model2 <- lm(mpg ~ acceleration + displacement, data = Auto)

aov_object <- aov(model2, data = Auto)
summary(aov_object)[[1]]
RSS <- 8371.7

fullmodel <- lm(mpg ~ horsepower + acceleration + displacement, data = Auto)
sigma_hat <- summary(full_model)$sigma

d = 2 # number of predictors
bic_2 <- RSS/sigma_hat^2 + d*log(n)
cp_2 <- (RSS + 2*(sigma_hat^2)*d)/n
bic_2
cp_2
```

## Problem 2: Subset Selection

### Part a (Code: 1 pt)

Run the code in ISLR Lab 6.5.1, "Best Subset Selection" and "Forward and Backward Stepwise Selection" subsections. (Do not run the "Choosing Among Models Using the Validation-Set Approach and Cross-Validation" section.)

```{r BSS p1}
library(ISLR2)
View(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```
```{r BSS p2}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```
```{r BSS p3}
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters)
summary(regfit.full)
```
```{r BSS p4}
regfit.full <- regsubsets(Salary ~ ., data = Hitters ,
nvmax = 19)
reg.summary <- summary(regfit.full)
```
```{r BSS p5}
names(reg.summary)
```
```{r BSS p6}
reg.summary$rsq
```
```{r BSS p7 & 8}
par(mfrow = c(2, 2))
plot(reg.summary$rss , xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$adjr2 , xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")

which.max(reg.summary$adjr2)
points (11, reg.summary$adjr2 [11] , col = "red", cex = 2,
pch = 20)
```
```{r BSS p9}
plot(reg.summary$cp, xlab = "Number of Variables",
ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points (10, reg.summary$cp[10] , col = "red", cex = 2,
pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic , xlab = "Number of Variables",
ylab = "BIC", type = "l")
points (6, reg.summary$bic [6], col = "red", cex = 2,
pch = 20)
```
```{r BSS p10}
plot(regfit.full , scale = "r2")
plot(regfit.full , scale = "adjr2")
plot(regfit.full , scale = "Cp")
plot(regfit.full , scale = "bic")
```
```{r BSS p11}
coef(regfit.full , 6)
```
```{r FBSS p1}
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters ,
nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters ,
nvmax = 19, method = "backward")
summary(regfit.bwd)
```
```{r FBSS p2}
coef(regfit.full , 7)
coef(regfit.fwd , 7)
coef(regfit.bwd , 7)
```

### Part b (Explanation: 1 pt)

Briefly explain how to interpret the plots created by `plot(regfit.full, scale = "some metric")` at the end of the Best Subset Selection section.

-> The x-axis represents the predicting variables and the y-axis represents the response variable. The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. The higher, the better the model.

### Part c (Code: 1 pt; Explanation: 1 pt)

In the rest of this problem, we will explore a situation in which the true model is *known* (more-or-less). In this true model, however, the error term is due to rounding and is *not* normally distributed, and there are some major collinearity issues. Let's see whether these violations of least-squares assumptions affect subset selection.

The madden17_QB dataset contains the overall rating (`OVR`) and individual skill ratings for 112 quarterbacks in the Madden NFL 2017 video game. According to an article on fivethirtyeight.com, the overall rating for quarterbacks is a linear combination of the following skill ratings: `AWR`, `THP`, `SAC`, `MAC`, `DAC`, `PAC`, `SPD`, `AGI`, `RUN`, and `ACC`. The other 34 skill ratings are not relevant.

Perform best subset selection on this dataset, using `nvmax = 10`. You may have to remove the categorical variables (`Name` and `Team`) in the formula or the dataset used to fit the model.

Did the algorithm correctly identify the 10 important variables in the model? If not, which variables were incorrectly left out, and which were incorrectly included?

```{r 2c}
madden17_QB1 <- madden17_QB[,3:47]
mad_regfit.full <- regsubsets(OVR ~ ., data = madden17_QB1 , nvmax = 10)
mad_reg.summary <- summary(mad_regfit.full)

plot(mad_regfit.full , scale = "r2")
plot(mad_regfit.full , scale = "adjr2")
plot(mad_regfit.full , scale = "Cp")
plot(mad_regfit.full , scale = "bic")
```
-> The algorithm correctly identifies the 10 important variables except for AIC, where it incorrectly included SFA.

### Part d (Code: 1 pt; Explanation: 1 pt)

Perform forward selection on this dataset, using `nvmax = 20`. How many variables are in the "best" model using BIC as a selection criterion? What about Cp? For the "best" model (using one of the criteria), which variables were incorrectly left out, and which were incorrectly included?

```{r 2d}
mad_regfit.fwd <- regsubsets(OVR ~ ., data = madden17_QB1 ,
nvmax = 20, method = "forward")
mad_regit.fwd_summary <- summary(mad_regfit.fwd)
mad_regit.fwd_summary$bic
mad_regit.fwd_summary$cp
```
-> "explain"

## Part e (Code: 1 pt; Explanation: 1 pt)

Perform backward selection on this dataset, using `nvmax = 20`. How many variables are in the "best" model using BIC as a selection criterion? What about Cp? For the "best" model (using one of the criteria), which variables were incorrectly left out, and which were incorrectly included?

```{r 2e}
mad_regfit.bwd <- regsubsets(OVR ~ ., data = madden17_QB1 ,
nvmax = 20, method = "backward")
mad_regit.bwd_summary <- summary(mad_regfit.bwd)
mad_regit.bwd_summary$bic
mad_regit.bwd_summary$cp
```
-> "explain"
