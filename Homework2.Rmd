---
title: 'Homework Assignment #2'
author: "Vanessa Avilez, Michael Bryant, Phuong Traceyle"
date: "Due March 2, 2023"
output: pdf_document
---

# Instructions

You should submit either two or three files:

1. You should write your solutions to the Simulation and Applied Problems in this R Markdown file and submit the (.Rmd) file.
2. You should knit the final solution file to pdf and submit the pdf. If you are having trouble getting code chunks to run, add `eval = FALSE` to the chunks that do not run. If you are having trouble getting R Studio to play nice with your LaTeX distribution, I will begrudgingly accept an HTML file instead. 
3. Solutions to the Key Terms and Conceptual Problems can be submitted in a separate Word or pdf file or included in the same files as your solutions to the Simulation and Applied Problems.

This homework assignment is worth a total of **40 points**.

```{r librares for applied}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(DescTools) #for application problem 2
```

# Key Terms (5 pts)
# 1,2,3,4 Phuong
# 5,6,7 Vanessa
# 8,9,10 Michael

Read Chapter 13 of Introduction to Statistical Learning, Second Edition. Based on your reading, answer the following questions.

1. What is a *p-value*? What is the difference between a one-sided and a two-sided p-value?
-> A p-value is the probability of observing a test statistic equal to or more extreme than the observed statistic, under the assumption that Ho is in fact true. A one-sided p-value is the probability of seeing such an extreme value of the test statistic. A two-sided p-value is the probability of seeing such an extreme value of the absolute test statistic.
2. In traditional NHST-style significance testing, what are the two possible decisions? When do we make each decision?
-> We either reject a null hypothesis if the p-value is less than $\alpha$ or fail to reject a null hypothesis if the p-value is greater than $\alpha$.
3. What is the difference between a *Type I Error* and a *Type II Error*?
-> A Type I Error occurs when we reject Ho when Ho is in fact true. A Type II Error occurs when we do not reject Ho when Ho is in fact false.
4. Briefly explain why it is necessary to adjust the significance level (or equivalently, the p-values) when testing a large number of null hypotheses.
-> It is necessary to adjust the significance level when testing a large number of null hypothesis because we are bound to get very small p-values. If we reject each null hypothesis without accounting for the fact that we have performed a large number of tests, we will end up with a lot of Type I Errors.
5. Compare and contrast the *Family-Wise Error Rate* (FWER) and the *False Discovery Rate* (FDR).
-> FWER and FDR are two different methods used to control type 1 errors. The difference is that FWER is the probability of making at least type 1 error while FDR is the proportion of type 1 errors that the model is allowed to make. FWER has four widely-used methods: Bonferroni, Holm's, Tukeys, and Scheffe's; while FDR has one widely-used method: Benjamini-Hochberg. It is difficult to contol for FDR since it requires the user to know the power and the number of null hypotheses that are false. 
6. Compare and contrast the *Bonferroni Method* and *Holm's Step-Down Method* for controlling the FWER.
->Bonferroni's and holm's step-down method are two ways to control FWER for m null hypotheses. They both have no assumptions about the null hypothesis, the type of test statistic used or the independence or dependence of the p-values. however Holm's is better than Bonferroni, because it is less conservative so it usually results in fewer type 2 errors. So holms has a greater power than bonferroni. 
7. Why do we prefer to use *Tukey's Method* or *Scheffe's Method* to control the FWER? In what conditions is it appropriate to use those methods instead of the Bonferroni or Holm methods?
-> We prefer to use Tukey's Method or Scheffe's Method to control FWER because in certain situations, we can achieve a higher power using these two methods over Bonferroni and holms. It is appropriate to use Tukey or Scheffes when some of the m p-values for the m pairwise comparisons are not independent. SO when we control for FWER, we can be less conservative to have a higher power.
8. Briefly describe the *Benjamini-Hochberg* procedure for controlling the FDR.
9. What is/are the major assumption(s) of a *permutation test*? What is the general procedure for obtaining the null distribution of a test statistic using a permutation test?
10. When is it useful/recommended to use a permutation testing approach as opposed to a traditional theory-based approach?

# Conceptual Problems
# Michael

## Conceptual Problem 1 (5 pts) 

Textbook Exercise 13.7.6
a) We have 7 false positives, 0 false negatives, 6 true positives, 17 true negatives, 0 Type I errors, and 7 Type II errors under Bonferroni.

b) We have 1 false positive, 0 false negatives, 6 true positives, 23 true negatives, 0 Type I error, and 1 Type II error under Holm.


## Conceptual Problem 2 (2 pts)

Suppose that we test $m = 1000$ independent null hypotheses, of which 10% are true, at significance level $\alpha = 0.05$ and achieve a false discovery rate of $q = 0.20$. Construct a table following Table 13.2 in the textbook, identifying the appropriate values of $V$, $S$, $U$, $W$, and $R$ in this situation.

## Conceptual Problem 3 (3 pts)

Suppose that we test $m = 1000$ independent null hypotheses, of which an unknown number $m_0$ are true, at significance level $\alpha = 0.05$. Suppose that each test also has a power of 0.80. Find and plot the false discovery rate as a function of $m_0$.

## Conceptual Problem 4 (2.5 pts)

Textbook Exercise 5.4.2 parts (a), (b), and (c).

# Simulation Problems 
# Phuong

## Simulation Problem 1 (Code: 1 pt; Explanation: 0.5 pts)

Textbook Exercise 5.4.2 parts (e), (g), and (h). For part (g), you should create a line plot (using either `plot` with argument `type = "l"` or `geom_line`). Then, to make clearer what you should be commenting on, find the limit as $n \rightarrow \infty$ of the probability that the $j^{th}$ observation is in your bootstrap sample and add a horizontal red line (using `abline` or `geom_hline`) at that value. (Hint: the limit as $n \rightarrow \infty$ of the expression in part (c) is well-known and easily found on the Internet.)

5.4.2 part e

-> When n = 100, 1 - (1 - 1/n)^n = 1 - (1 - 1/100)^100, which is approximately 0.634.

```{r 5.4.2 part g}
n = 1:100000
plot(n, 1 - (1 - 1/n)^n, typ = "l")
abline(h = 1 - exp(-1), col = "red")
```
-> The limit of (1 - 1/n)^n as $n \rightarrow \infty$ is exp(-1). As n gets larger, we see that 1 - (1 - 1/n)^n approaches 1 - exp(-1).

```{r 5.4.2 part h}
store <- rep(NA, 10000)
for (i in 1:10000){
  store[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0
}
mean(store)
```
-> We can see that the result is approximately equal to 1 - (1 - 1/100)^100, which is close to 1 - exp(-1).

## Simulation Problem 2 (Code: 1.5 pts; Explanation: 3.5 pts)

Copy the *functions* you created in the Bootstrap Confidence Intervals class activity as well as Simulation Parts 3, 4, and 5.

Write a brief summary of what you learned from the activity. Make sure to address the following questions:

- Are theory-based methods *guaranteed* to achieve the appropriate coverage? What about bootstrap-based methods?
- Which of the four methods appear to be range-preserving even in a "worst-case scenario"? 
- When and why would a bootstrap method be useful to obtain a confidence interval even if it doesn't achieve the appropriate coverage?

```{r bootstrap resample function}
bootstrap_resample <- function(data_vector, B, summary_fn = mean,seed=100, ...){
  # data_vector: a vector of data
  # B: the number of bootstrap resamples
  # summary_fn: the name of a function to apply to the resampled data
  # ...: any additional arguments to summary_fn
  
  boot_samples <- matrix(0, nrow = length(data_vector), ncol = B)
  
  n <- length(data_vector)
  # We need to add some code here!
  set.seed(seed)
  
  for (i in 1:B) {
    boot_samples[,i] <- sample(data_vector, size = n, replace = TRUE)
  }
  
  boot_stat <- apply(boot_samples, 2, summary_fn, ...)
  # Seriously, do not name any arguments x or FUN when using apply within a function
  # Advice from someone who spent 30 minutes debugging a sample solution
  
  return(boot_stat)
}
```

Then we can create our confidence interval from the bootstrap-resampled estimates. You need to fill in the code in the `if/else` section to compute the bootstrap confidence interval the appropriate way.

```{r bootstrap CI function}
bootstrap_ci <- function(data_vector, method, B = 1000, seed = 100, C = 0.95, summary_fn = mean, ...){
  # data_vector: a vector of data
  # method: the CI method
  # B: the number of bootstrap resamples
  # seed: the seed to use
  # C: the confidence level as a decimal
  # summary_fn: the name of a function to apply to the resampled data
  # ...: any additional arguments to summary_fn
  
  obs_stat <- do.call(summary_fn, args = list(data_vector, ...))
  # do.call allows you to call a function without having to hard-code what that function is
  # the args argument is a list of arguments to the function
  # so this will find the observed value of the statistic given the original data vector
  
  bootstrap_values <- bootstrap_resample(data_vector, B, summary_fn, ...)
  
  alpha <- 1 - C
  if (method == "percentile"){
      # write code to get the percentile confidence level out of the returned bootstrap_values and store in a length 2 vector boot_ci
    boot_ci <- quantile(bootstrap_values, probs = c(alpha/2,1-alpha/2))
  } else if (method == "basic"){
    # write code to get the "basic" confidence level and store in a length 2 vector boot_ci
    boot_ci <- 2*obs_stat - quantile(bootstrap_values, probs = c(1-alpha/2 , alpha /2))
  } else if (method == "normal"){
    # write code to get the normal-theory confidence interval and store in a length 2 vector boot_ci
    center <- 2 * obs_stat - mean(bootstrap_values)
    crit_value <- qnorm(alpha/2)
    se_boot <- sd(bootstrap_values)
    boot_ci <- center + c(1, -1)*crit_value*se_boot
    # make sure to use the adjustments in the course notes, e.g., don't use 1/sqrt(n) as your standard deviation of sample means
  }
  
  return(boot_ci)
}
```

Simulation Part 3

```{r sim 3}
set.seed(437)
# need to do the simulation now!
sim_data3 <- matrix(rexp(1000*100), nrow = 1000, ncol = 100)
```

```{r t-CI3}
# You should be able to just run this code chunk without any fixes
pop_mean3 <- 1
ci_t3 <- apply(sim_data3, 1, function(x) t.test(x)$conf.int)
ci_t_df3 <- as.data.frame(t(ci_t3))
names(ci_t_df3) <- c("lower", "upper")
ci_t_df3 %>% mutate(covered = lower <= pop_mean3 & upper >= pop_mean3) %>%
  summarize(coverage_probability = mean(covered))
```

```{r boot percentile CI 3}
# You should be able to just run this code chunk without any fixes
ci_perc3 <- apply(sim_data3, 1, bootstrap_ci, method = "percentile", B = 1000, summary_fn = mean, na.rm = TRUE)
# don't need any ... arguments here, but illustrating the idea of the ...
# notice that bootstrap_ci has default seed = 100 and C = 0.95 arguments
ci_perc_df3 <- as.data.frame(t(ci_perc3))
names(ci_perc_df3) <- c("lower", "upper")
ci_perc_df3 %>% mutate(covered = lower <= pop_mean3 & upper >= pop_mean3) %>%
  summarize(coverage_probability = mean(covered))
```

Copy and modify the chunk above for the basic and normal-theory intervals.

```{r boot basic CI 3}
# You should be able to just run this code chunk without any fixes
ci_basic3 <- apply(sim_data3, 1, bootstrap_ci, method = "basic", B = 1000, summary_fn = mean, na.rm = TRUE)
# don't need any ... arguments here, but illustrating the idea of the ...
# notice that bootstrap_ci has default seed = 100 and C = 0.95 arguments
ci_basic_df3 <- as.data.frame(t(ci_basic3))
names(ci_basic_df3) <- c("lower", "upper")
ci_basic_df3 %>% mutate(covered = lower <= pop_mean3 & upper >= pop_mean3) %>%
  summarize(coverage_probability = mean(covered))
```

```{r boot normal CI 3}
# You should be able to just run this code chunk without any fixes
ci_norm3 <- apply(sim_data3, 1, bootstrap_ci, method = "normal", B = 1000, summary_fn = mean, na.rm = TRUE)
# don't need any ... arguments here, but illustrating the idea of the ...
# notice that bootstrap_ci has default seed = 100 and C = 0.95 arguments
ci_norm_df3 <- as.data.frame(t(ci_norm3))
names(ci_norm_df3) <- c("lower", "upper")
ci_norm_df3 %>% mutate(covered = lower <= pop_mean3 & upper >= pop_mean3) %>%
  summarize(coverage_probability = mean(covered))
```

Simulation Part 4

```{r sim 4}
set.seed(437)
# need to do the simulation now!
sim_data4 <- matrix(rexp(1000*10), nrow = 1000, ncol = 10)
```

```{r t-CI4}
# You should be able to just run this code chunk without any fixes
pop_mean4 <- 1
ci_t4 <- apply(sim_data4, 1, function(x) t.test(x)$conf.int)
ci_t_df4 <- as.data.frame(t(ci_t4))
names(ci_t_df4) <- c("lower", "upper")
ci_t_df4 %>% mutate(covered = lower <= pop_mean4 & upper >= pop_mean4) %>%
  summarize(coverage_probability = mean(covered))
```

```{r boot percentile CI 4}
# You should be able to just run this code chunk without any fixes
ci_perc4 <- apply(sim_data4, 1, bootstrap_ci, method = "percentile", B = 1000, summary_fn = mean, na.rm = TRUE)
# don't need any ... arguments here, but illustrating the idea of the ...
# notice that bootstrap_ci has default seed = 100 and C = 0.95 arguments
ci_perc_df4 <- as.data.frame(t(ci_perc4))
names(ci_perc_df4) <- c("lower", "upper")
ci_perc_df4 %>% mutate(covered = lower <= pop_mean4 & upper >= pop_mean4) %>%
  summarize(coverage_probability = mean(covered))
```

Copy and modify the chunk above for the basic and normal-theory intervals.

```{r boot basic CI 4}
# You should be able to just run this code chunk without any fixes
ci_basic4 <- apply(sim_data4, 1, bootstrap_ci, method = "basic", B = 1000, summary_fn = mean, na.rm = TRUE)
# don't need any ... arguments here, but illustrating the idea of the ...
# notice that bootstrap_ci has default seed = 100 and C = 0.95 arguments
ci_basic_df4 <- as.data.frame(t(ci_basic4))
names(ci_basic_df4) <- c("lower", "upper")
ci_basic_df4 %>% mutate(covered = lower <= pop_mean4 & upper >= pop_mean4) %>%
  summarize(coverage_probability = mean(covered))
```

```{r boot normal CI 4}
# You should be able to just run this code chunk without any fixes
ci_norm4 <- apply(sim_data4, 1, bootstrap_ci, method = "normal", B = 1000, summary_fn = mean, na.rm = TRUE)
# don't need any ... arguments here, but illustrating the idea of the ...
# notice that bootstrap_ci has default seed = 100 and C = 0.95 arguments
ci_norm_df4 <- as.data.frame(t(ci_norm4))
names(ci_norm_df4) <- c("lower", "upper")
ci_norm_df4 %>% mutate(covered = lower <= pop_mean4 & upper >= pop_mean4) %>%
  summarize(coverage_probability = mean(covered))
```

Simulation Part 5

```{r hist }
hist(x = ci_t_df4$lower)
hist(ci_perc_df4$lower)
hist(ci_basic_df4$lower)
hist(ci_norm_df4$lower)
```

-> The theory-based and bootstrap-based methods are not guaranteed to achieve the appropriate coverage. The only method that is range-preserving is percentile because the others give p-values less than 0. A bootstrap method is useful to obtain a confidence interval when there is a very large sample because we don't have to rely on invalid assumptions of a dataset.

# Applied Problems
# Vanessa

## Applied Problem 1 (Code: 4 pts; Explanation: 2 pts)

Using the `dplyr` package, subset the `mpg` dataset from the `ggplot2` package to include only the cars from 2008 that are minivans, pickups, or SUVs (`%in%` is a useful replacement for `==` when trying to match to more than one possibility).

```{r subset of mpg}
mpg_sub <- mpg %>% filter(class %in% c('minivan','pickup','suv'), year == 2008)
mpg_sub1 <- mpg %>% filter(class %in% c('minivan','pickup'), year == 2008)
mpg_sub2 <- mpg %>% filter(class %in% c('pickup','suv'), year == 2008)
mpg_sub3 <- mpg %>% filter(class %in% c('minivan','suv'), year == 2008)
```

Using this new dataset, determine which of the following statements is/are true, using an $\alpha = 0.10$ significance level/family-wise error rate or a $q = 0.10$ false discovery rate:

1. There is a significant difference in highway gas mileage between minivans and SUVs.
2. There is a significant difference in highway gas mileage between pickups and SUVs.
3. There is a significant difference in highway gas mileage between minivans and pickups.

Use the following methods.

(a) Three two-sample t-tests with no adjustments for multiple testing. Store all three p-values in a single vector so that you can use the `p.adjust` function in later parts. 

```{r prob 2A}
p1 <- t.test(formula = hwy ~ class, data = mpg_sub1, alternative = "g",var.equal = TRUE)$p.value
p2 <- t.test(formula = hwy ~ class, data = mpg_sub2, alternative = "g",var.equal = TRUE)$p.value
p3 <- t.test(formula = hwy ~ class, data = mpg_sub3, alternative = "g",var.equal = TRUE)$p.value

p_all <- c(p1,p2,p3)
p_all

sum (p_all <= .1)
```
At $\alpha = 0.10$, we can say that 1 and 3 are true. 

(b) Three two-sample t-tests followed by Bonferroni's method. 

```{r prob 2b}
p1 <- t.test(formula = hwy ~ class, data = mpg_sub1, alternative = "g",var.equal = TRUE)$p.value
p2 <- t.test(formula = hwy ~ class, data = mpg_sub2, alternative = "g",var.equal = TRUE)$p.value
p3 <- t.test(formula = hwy ~ class, data = mpg_sub3, alternative = "g",var.equal = TRUE)$p.value

p_all <- c(p1,p2,p3)
p_all

q.values.Bon <- p.adjust(p_all, method = "bonferroni")
q.values.Bon

sum (q.values.Bon <= .1)

```
At $\q = 0.10$, we can say that 1 and 3 are true. 

(c) Three two-sample t-tests followed by Holm's step-down method.

```{r prob 2c}
p1 <- t.test(formula = hwy ~ class, data = mpg_sub1, alternative = "g",var.equal = TRUE)$p.value
p2 <- t.test(formula = hwy ~ class, data = mpg_sub2, alternative = "g",var.equal = TRUE)$p.value
p3 <- t.test(formula = hwy ~ class, data = mpg_sub3, alternative = "g",var.equal = TRUE)$p.value

p_all <- c(p1,p2,p3)
p_all


q.values.H <- p.adjust(p_all, method = "holm")
q.values.H

sum (q.values.H <= .1)
```
At $\q = 0.10$, we can say that 1 and 3 are true. 

(d) A one-way ANOVA followed by Tukey's method.

```{r prob 2d}
p1a <- aov(formula = hwy ~ class, data = mpg_sub1)
p2a <- aov(formula = hwy ~ class, data = mpg_sub2)
p3a <- aov(formula = hwy ~ class, data = mpg_sub3)

p_all_anova <- c(p1a,p2a,p3a)

TukeyHSD(x = p1a)
TukeyHSD(x = p2a)
TukeyHSD(x = p3a)
```
At $\q = 0.10$, we can say that 1,2, and 3 are true. 

(e) Three two-sample t-tests followed by the Benjamini-Hochberg (BH) method.

```{r prob 2e}
p1 <- t.test(formula = hwy ~ class, data = mpg_sub1, alternative = "g",var.equal = TRUE)$p.value
p2 <- t.test(formula = hwy ~ class, data = mpg_sub2, alternative = "g",var.equal = TRUE)$p.value
p3 <- t.test(formula = hwy ~ class, data = mpg_sub3, alternative = "g",var.equal = TRUE)$p.value

p_all <- c(p1,p2,p3)
p_all


q.values.BH <- p.adjust(p_all, method = "BH")
q.values.BH

sum (q.values.BH <= .1)
```
At $\q = 0.10$, we can say that 1 and 3 are true. 

Compare and contrast your results.
For all except Tukey's Method, we could say that there is a significant difference in minivan and suvs, and there is a significant difference in minivans and pickups. For Tukey's method we could say that there is a significant differnce in minivans and suvs, minivans and pickups, and pickups and suvs. 


## Applied Problem 2 (Code: 1 pt; Explanation: 1 pt)

Use a one-way ANOVA followed by Scheffe's method (`ScheffeTest` in the DescTools package) to determine whether the following statement is true at the $\alpha = 0.10$ significance level:

There is a significant difference in highway gas mileage between pickups and non-pickups (SUVs and minivans).

```{r ANOVA }
#remember to add DescTools thru packages
#library(DescTools)

p3a <- aov(formula = hwy ~ class, data = mpg_sub3)

ScheffeTest(x = p3a)
```
The statement "There is a significant difference in highway gas mileage between pickups and non-pickups (SUVs and minivans)" is true. 

## Applied Problem 3 (Code: 5 pts; Explanation: 3 pts)

Textbook Exercise 5.4.9.

```{r ISLR 5.4.9 exercise a}
#a) mean of medv
medv_muhat <- mean(Boston$medv)
medv_muhat
```

```{r ISLR 5.4.9 exercise b}
#b) standard error of muhat
se_muhat <- sd(Boston$medv)/sqrt(length(Boston$medv))
se_muhat
```

```{r ISLR 5.4.9 exercise c}
#c) bootstrap the mean
muhat_boot <- numeric(10000)
for (i in 1:10000) {
  boot_samp <- sample(Boston$medv, size = length(Boston$medv),replace = T)
  muhat_boot[i] <- mean(boot_samp)
}
se_muhat_boot <- sd(muhat_boot)
se_muhat_boot
```

this number is pretty close to the number we got in part b, so its most likely correct.

```{r ISLR 5.4.9 exercise d}
# d) confidence interval using bootstrap se
low_int <- medv_muhat - 2* se_muhat_boot
up_int <- medv_muhat + 2* se_muhat_boot
low_int
up_int
t.test(Boston$medv)
```

```{r ISLR 5.4.9 exercise e}
# e) median of Boston medv
muhat_med <- median(Boston$medv)
muhat_med
```

```{r ISLR 5.4.9 exercise f}
# f) bootstrap the median
muhat_boot2 <- numeric(10000)
for (i in 1:10000) {
  boot_samp2 <- sample(Boston$medv, size = length(Boston$medv),replace = T)
  muhat_boot2[i] <- median(boot_samp2)
}
se_muhat_boot2 <- sd(muhat_boot2)
se_muhat_boot2
```

the standard error of the bootstrapped median samples is 0.3808. We estimate with 95% confidence that the population median of 'medv' is in the interval [22.53 - 0.7581, 22.53 + 0.7581]

```{r ISLR 5.4.9 exercise g}
# g) estimate for the tenth percentile of medv
muhat_0.1 <- quantile(Boston$medv, probs = .1)
muhat_0.1
```

```{r ISLR 5.4.9 exercise h}
# h) bootstrap the tenth quantile
muhat_boot3 <- numeric(10000)
for (i in 1:10000) {
  boot_samp3 <- sample(Boston$medv, size = length(Boston$medv),replace = T)
  muhat_boot3[i] <- quantile(boot_samp3,probs = .1)
}
se_muhat_boot3 <- sd(muhat_boot3)
se_muhat_boot3

```

the standard error of the bootstrapped tenth quantile samples is 0.3808. We estimate with 95% confidence that the tenth quantile of 'medv' is in the interval [22.53 - 1.0026, 22.53 + 1.0026].



